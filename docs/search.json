[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataSHIELD Workshop",
    "section": "",
    "text": "This website has been created to host the materials and exercises for the ‘Utilization of the unCoVer toolbox for covidー19 data analysis’, hosted at the UPM-Montegancedo Campus the 6th of May, 2022.\nOn it you will find reading materials, setup tutorials, the workshop indications and practical exercises.\n\n\nBefore the workshop we suggest the atendants to take a look at the “Environment setup” and “Get up to speed” sections. This way they will have their computers with the right software installed to follow the workshop.\n\n\n\n[TABLE OF THE WORKSHOP SCHEDULE]\n\n\n\nThis project is funded by the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement No 101016216. Developed at ISGlobal-Barcelona (BRGE) by Juan R. González and Xavier Escribà Montagut"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Environment setup",
    "section": "",
    "text": "Along this section, we will go over the R packages (and versions) required to navigate through the workshop. We recommend to use RStudio as the integrated development environment (IDE) to use R, although any other IDE can be used. We suggest to use R version > 4.0."
  },
  {
    "objectID": "setup.html#r-packages",
    "href": "setup.html#r-packages",
    "title": "Environment setup",
    "section": "R packages",
    "text": "R packages\nThe required R packages are the following:\n\nDSI 1.4.0: The DataSHIELD Interface (DSI) handles the connections to the databases.\nDSOpal 1.3.0: DSOpal is an extension of DSI to connect to to Opal servers.\ndsBaseClient 6.1.1: Implementation of the base R functions (Example: Base package function as.factor is implemented as ds.asFactor).\ndsHelper 0.4.12: dsHelper is a compedium of wrappers for dsBaseClient functions that simplify some common operations to maintain sanity."
  },
  {
    "objectID": "setup.html#install-guide",
    "href": "setup.html#install-guide",
    "title": "Environment setup",
    "section": "Install guide",
    "text": "Install guide\n\n\n\n\n\n\nThe proposed install guide has been tested on a clean installation with R 4.0.4 and RStudio 2022.02.2 Build 485; if any errors occur, please consider using a clean install or refer to the official R documentation regarding the errors you are getting. If you are facing permissions issues, contact your system administrator.\n\n\n\nTo install the required packages we will need the devtools package.\n\ninstall.packages(\"devtools\")\n\nAfterwards, we can install the packages:\n\ndevtools::install_github(\"datashield/dsBaseClient\", \"6.1.1\")\ndevtools::install_version(\"DSI\", version = \"1.4.0\")\ndevtools::install_version(\"DSOpal\", version = \"1.3.0\")\ndevtools::install_github(\"lifecycle-project/ds-helper\", \"0.4.12\")\n\n\n\n\nInstall console"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Get up to speed: Useful reads",
    "section": "",
    "text": "Along this workshop, there are some details regarding DataSHIELD and “resources” that are not explained in detail, it is expected that the reader is familiar with them. If that is not the case, there are other free online books/papers with that knowledge.\n\nDataSHIELD paper: Description of what is DataSHIELD.\nDataSHIELD wiki: Materials about DataSHIELD including:\n\nBeginner material\nRecorded DataSHIELD workshops\nInformation on current release of DataSHIELD\n\nresource book: In this book you will find information about:\n\nDataSHIELD (Section 5)\nWhat are resources (Section 6/7)"
  },
  {
    "objectID": "help.html#opal",
    "href": "help.html#opal",
    "title": "Get up to speed: Useful reads",
    "section": "Opal",
    "text": "Opal\nWe will be interacting with DataSHIELD through a data warehouse called Opal. This is the server that will handle the authentication of our credentials, storage of data and “resources” and will provide an R server where the non-disclosive analysis will be conducted. Information about it can also be foun online:\n\nOpal papers 1; 2\nOpal documentation"
  },
  {
    "objectID": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "href": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "title": "Get up to speed: Useful reads",
    "section": "“resources”: A very simple explanation without any technicalities",
    "text": "“resources”: A very simple explanation without any technicalities\nIt is quite important to have a solid understanding of what are the “resources” and how we work with them, since we will be using them to load our data on the R sessions. For that reason we included a very brief description of them without using technicalities.\nThe “resources” can be imagined as a data structure that contains the information about where to find a data set and the access credentials to it; we as DataSHIELD users are not able to look at this information (it is privately stored on the Opal server), but we can load it into our remote R session to make use of it. Following that, the next step comes naturally.\nOnce we have in an R session the information to access a dataset (an table for example) we have to actually retrieve it on the remote R session to analyze it. This step is called resolving the resource.\nThose two steps can be identified on the code we provide as the following:\nLoading the information of a “resource”:\n\nDSI::datashield.assign.resource(conns, \"resource\", \"resource.path.in.opal.server\")\n\nResolving the “resource”:\n\nDSI::datashield.assign.expr(conns, \"resource.resolved\", expr = as.symbol(\"as.resource.data.frame(resource)\"))\n\nThis toy code would first load the “resource” on a variable called resource and it would retrieve the information it contains and assign it to a variable called resource.resolved."
  },
  {
    "objectID": "workshop_part0.html",
    "href": "workshop_part0.html",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "",
    "text": "To begin, we will load the libraries that will allow us to connect to the Opal analysis servers.\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)"
  },
  {
    "objectID": "workshop_part0.html#log-in-to-the-study-servers",
    "href": "workshop_part0.html#log-in-to-the-study-servers",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "Log in to the study servers",
    "text": "Log in to the study servers\nWe begin by creating the connection object, to do that we will use the DSI package. It is at this stage that we can decide whether only connect to a single study server or to multiple. If we connect to multiple study servers we can make use of the pooled functionalities of some DataSHIELD functions.\n\nbuilder <- DSI::newDSLoginBuilder()\nbuilder$append(server = \"study1\",\n               url = \"https://192.168.1.200:8005\",\n               user = \"user_analisis\", password = \"Ekfl07UUgz\")\nlogindata <- builder$build()\n\nWe just created the logindata object, which contains all the login information, the next step is to use this information and actually connect to the servers.\n\nconnections <- DSI::datashield.login(logins = logindata)\n\n\nLogging into the collaborating servers\n\n\nError in curl::curl_fetch_memory(url, handle = handle): schannel: next InitializeSecurityContext failed: SEC_E_UNTRUSTED_ROOT (0x80090325) - La cadena de certificación fue emitida por una entidad en la que no se confía.\n\n\nThe error we are getting is due the study server having no SSL certificate, by default R refuses to connect to endpoints without SSL certificates. We do not need to know what an SSL certificate is or why is it important.\nWe can maually disable this limitation of R so that it accepts to connect.\n\nlibrary(httr);set_config(config(ssl_verifypeer = 0L))\n\nAnd now we successfully connect to the study servers.\n\nconnections <- DSI::datashield.login(logins = logindata)\n\n\nLogging into the collaborating servers"
  },
  {
    "objectID": "workshop_part1.html",
    "href": "workshop_part1.html",
    "title": "Part 1: Loading the data",
    "section": "",
    "text": "Now that we are connected to the study servers, we have to load the data on the remote R sessions."
  },
  {
    "objectID": "workshop_part1.html#loading-the-resources",
    "href": "workshop_part1.html#loading-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Loading the resources",
    "text": "Loading the resources\nWe will begin by loading the resources. If we are not sure which resources has each study server, we can go to the user interface of the Opals. To do that, just go to your browser of choice and navigate to the server URL. Login with the same credentials you are using for DataSHIELD.\n\n\n\nOpal UI lading page\n\n\nOnce we login, we have to navigate to the Projects tab, there we will find the available projects on the Opal server, we have to write down the name of the project we are interested on using.\n\n\n\nOpal UI Projects page\n\n\nWe then click on the project of interest. On the resources tab we will see the available resources. We have to write down the resource of interest.\n\n\n\nOpal UI Available resources\n\n\nWe can therefore conclude that on the illustrated example, the information of interest is:\n\nURL: https://192.168.1.200:8005\nProject name: UMF_Cluj\nResource name: Romania\n\nWith this information, we can go back to the RStudio and load the resources in the remote R sessions.\n\nDSI::datashield.assign.resource(connections, \"resource\", \"UMF_Cluj.Romania\")"
  },
  {
    "objectID": "workshop_part1.html#resolving-the-resources",
    "href": "workshop_part1.html#resolving-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Resolving the resources",
    "text": "Resolving the resources\nUp to this point we have created an object called resource that contains all the information required to load the data to the R session. In order do so, we just have to “resolve” this object. We do that with the following function.\n\nDSI::datashield.assign.expr(conns = connections, symbol = \"data\", \n                            expr = \"as.resource.data.frame(resource)\")\n\nNow we have created an object called data. This object contains a dataframe with the data we will use to perform our analysis. We can check if that is true and the dimensions of this dataframe.\n\nds.class(\"data\")\n\n$study1\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nds.dim(\"data\")\n\n$`dimensions of data in study1`\n[1] 999 132\n\n$`dimensions of data in combined studies`\n[1] 999 132"
  },
  {
    "objectID": "workshop_part2.html",
    "href": "workshop_part2.html",
    "title": "Part 2: Data validation",
    "section": "",
    "text": "Now that we have the data loaded, we can check if the data is compliant with the ranges described on the codebook. (IS IT PUBLIC/CAN WE LINK IT HERE?). On this step we can check that the categorical variables are properly encoded and that the numerical variables have the appropriate range. Regarding the numerical variables, we will not be checking the range, we will be looking the 5%/95% interquartile range, as there is no DataSHIELD function to output maximums and minimums."
  },
  {
    "objectID": "workshop_part2.html#columns-available",
    "href": "workshop_part2.html#columns-available",
    "title": "Part 2: Data validation",
    "section": "Columns available",
    "text": "Columns available\nFirst, we check the columns available on the loaded data.\n\nds.colnames(\"data\")\n\n$study1\n  [1] \"id\"               \"DMRBORN\"          \"DMRAGEYR\"        \n  [4] \"DATAD\"            \"DATSO\"            \"SMXASAH\"         \n  [7] \"SMXRNA\"           \"SMXSTA\"           \"SMXCOA\"          \n [10] \"SMXCPA\"           \"SMXSBA\"           \"SMXWHA\"          \n [13] \"SMXFEA\"           \"SMXHEA\"           \"SMXFAA\"          \n [16] \"SMXACA\"           \"SMXSEA\"           \"SMXSLA\"          \n [19] \"SMXANA\"           \"SMXNAA\"           \"SMXDIA\"          \n [22] \"SMXAPA\"           \"SMXMYA\"           \"SMXARA\"          \n [25] \"SMXSRA\"           \"SMXBLA\"           \"CMXCVD\"          \n [28] \"CMXHT\"            \"CMXCPD\"           \"CMXASM\"          \n [31] \"CMXCKD\"           \"RFXOB\"            \"CMXCLD\"          \n [34] \"CMXAP\"            \"CMXCND\"           \"RFXONC\"          \n [37] \"RFXHIV\"           \"CMXDI\"            \"CMXRHE\"          \n [40] \"RFXTB\"            \"RFXSM\"            \"CMXPU\"           \n [43] \"CSXCOT\"           \"CSXBTPA\"          \"CSXCHRA\"         \n [46] \"CSXSYA\"           \"CSXDIA\"           \"CSXOSTA\"         \n [49] \"DATIMD\"           \"IMDXXR\"           \"IMDXCTTE\"        \n [52] \"DATLBDHn\"         \"LBXCRPHn\"         \"LBXESRHn\"        \n [55] \"LBXSPCHn\"         \"LBXFERSIHn\"       \"LBXCDDHn\"        \n [58] \"LBXPHHn\"          \"LBXPOHn\"          \"LBXPCOHn\"        \n [61] \"LBXSC3SIHn\"       \"LBXBEHn\"          \"LBXSBLHn\"        \n [64] \"TRXAC\"            \"TRXCH\"            \"TRXIS\"           \n [67] \"TRXIM\"            \"TRXAB\"            \"TRXAV\"           \n [70] \"TRXVC\"            \"TRXVD\"            \"TRXZN\"           \n [73] \"TRXCS\"            \"TRXOX\"            \"TRXIT\"           \n [76] \"TRXTR\"            \"TRXNIV\"           \"DSXIC\"           \n [79] \"DSXOS\"            \"DATLGT\"           \"SMXRNA_numeric\"  \n [82] \"SMXSTA_numeric\"   \"SMXCOA_numeric\"   \"SMXCPA_numeric\"  \n [85] \"SMXSBA_numeric\"   \"SMXWHA_numeric\"   \"SMXHEA_numeric\"  \n [88] \"SMXFAA_numeric\"   \"SMXACA_numeric\"   \"SMXSEA_numeric\"  \n [91] \"SMXSLA_numeric\"   \"SMXANA_numeric\"   \"SMXNAA_numeric\"  \n [94] \"SMXDIA_numeric\"   \"SMXAPA_numeric\"   \"SMXMYA_numeric\"  \n [97] \"SMXARA_numeric\"   \"SMXSRA_numeric\"   \"SMXBLA_numeric\"  \n[100] \"CMXCVD_numeric\"   \"CMXHT_numeric\"    \"CMXCPD_numeric\"  \n[103] \"CMXASM_numeric\"   \"CMXCKD_numeric\"   \"RFXOB_numeric\"   \n[106] \"CMXCLD_numeric\"   \"CMXAP_numeric\"    \"CMXCND_numeric\"  \n[109] \"RFXONC_numeric\"   \"RFXHIV_numeric\"   \"CMXDI_numeric\"   \n[112] \"CMXRHE_numeric\"   \"RFXTB_numeric\"    \"RFXSM_numeric\"   \n[115] \"CMXPU_numeric\"    \"IMDXCTTE_numeric\" \"TRXAC_numeric\"   \n[118] \"TRXCH_numeric\"    \"TRXIS_numeric\"    \"TRXIM_numeric\"   \n[121] \"TRXAB_numeric\"    \"TRXAV_numeric\"    \"TRXVC_numeric\"   \n[124] \"TRXVD_numeric\"    \"TRXZN_numeric\"    \"TRXCS_numeric\"   \n[127] \"TRXOX_numeric\"    \"TRXIT_numeric\"    \"TRXTR_numeric\"   \n[130] \"TRXNIV_numeric\"   \"DSXIC_numeric\"    \"DSXOS_numeric\""
  },
  {
    "objectID": "workshop_part2.html#categorical-variables",
    "href": "workshop_part2.html#categorical-variables",
    "title": "Part 2: Data validation",
    "section": "Categorical variables",
    "text": "Categorical variables\nWe compare against the codebook and see that we have the SMXAPA variable, which should have the categories: 0 and 1 (plus missing).\n \nFirst, we can check the class of this variable.\n\nds.class(\"data$SMXAPA\")\n\n$study1\n[1] \"character\"\n\n\nWe might have expected to have a variable of class factor, later on this workshop we will see how to transform a variable to a factor, for the moment we do not have to worry. Having a character variable is enough for the data validation.\nFollowing that, we can extract an uni-dimensional contingency table of the variable. This way we will obtain the different categories of the variable and their counts. Aside, we will obtain the count of missings (NA) for this variable.\n\nds.table(\"data$SMXAPA\")$output.list$TABLES.COMBINED_all.sources_counts\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\n\n\ndata$SMXAPA\n No Yes  NA \n236  16 747 \n\n\nThis variable complies with the codebook."
  },
  {
    "objectID": "workshop_part2.html#numerical-variables",
    "href": "workshop_part2.html#numerical-variables",
    "title": "Part 2: Data validation",
    "section": "Numerical variables",
    "text": "Numerical variables\nFrom previously shown codebook screenshot, we can see that we also have the DMRAGEYR variable, which should be numeric. We can confirm that.\n\nds.class(\"data$DMRAGEYR\")\n\n$study1\n[1] \"numeric\"\n\n\nAs previously mentioned, we will check the 5%/95% interquartile range to see the range of the variable.\n\nds.summary(\"data$DMRAGEYR\")\n\n$study1\n$study1$class\n[1] \"numeric\"\n\n$study1$length\n[1] 999\n\n$study1$`quantiles & mean`\n    5%    10%    25%    50%    75%    90%    95%   Mean \n40.450 46.900 54.250 65.000 72.000 80.100 84.000 63.832 \n\n\nThis is not the most useful way to check the range of a variable. There is an alternative provided by the dsHelper package, which is to exatract the (noise-perturbed) variable which would be used for the scatter plot. Depending on the security parameters of the Opal server, the noise added could be too severe to be useful, but with the default settings it serves as a great alternative.\n\nvariable <- dh.getAnonPlotData(df = \"data\", var_1 = \"DMRAGEYR\")\nhead(variable)\n\n# A tibble: 6 x 2\n  cohort DMRAGEYR\n  <chr>     <dbl>\n1 study1     64.0\n2 study1     61.0\n3 study1     65.0\n4 study1     49.9\n5 study1     76.1\n6 study1     66.0\n\nrange(variable[,2])\n\n[1] 30.85094 91.79304\n\n\nFinally, we can also check the amount of missings of the numerical variable.\n\nds.numNA(\"data$DMRAGEYR\")\n\n$study1\n[1] 749"
  },
  {
    "objectID": "workshop_part4.html",
    "href": "workshop_part4.html",
    "title": "Part 3: Data wranggling",
    "section": "",
    "text": "We are now in the position to start working with the data. The first step is to learn how to manipulate it."
  },
  {
    "objectID": "workshop_part4.html#data-classes",
    "href": "workshop_part4.html#data-classes",
    "title": "Part 3: Data wranggling",
    "section": "Data classes",
    "text": "Data classes\nAll the tools to perform data transformations are the following:\n\nds.asCharacter: Useful to convert numerical values that we want as characters.\nds.asDataMatrixr: Useful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Maintains the original class for all columns.\nds.asMatrixUseful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Converts all columns into character class.\nds.asFactor: May cause disclosure issues if we try to convert a continuous variable. Very useful when the loaded categorical variables are represented as character instead of factor.\nds.asInteger: Useful when we require integer values, as the class numeric can’t guarantee it.\nds.asList: Rarely used.\nds.asLogical: To have bool variables. Similar to having factor variables.\nds.asNumeric: Useful to convert columns that have been interpreted as character class but we want them as numbers.\n\nWhen we use all this functions, we will create a new object on the study servers. If we are dealing with a data.frame and want to include this new column we created into it, we can use the following code.\n\nds.asFactor(input.var.name = \"data$SMXRNA\", newobj.name = \"SMXRNA_factor\")\n\n$all.unique.levels\n[1] \"No\"  \"Yes\"\n\n$return.message\n[1] \"Data object <SMXRNA_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, SMXRNA_factor)\")\n\nWe will overwrite the input data object with an added column.\n\ntail(ds.colnames(\"data\")[[1]])\n\n[1] \"TRXIT_numeric\"  \"TRXTR_numeric\"  \"TRXNIV_numeric\" \"DSXIC_numeric\" \n[5] \"DSXOS_numeric\"  \"SMXRNA_factor\""
  },
  {
    "objectID": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "href": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "title": "Part 3: Data wranggling",
    "section": "Complex recoding. Number of comorbidities",
    "text": "Complex recoding. Number of comorbidities\nGiven the available variables on our dataset, we might be interested on recoding variables or creating new variables as combinations of the existing ones. This is exactly what we will do here. We will take the comorbities: RFXOB, CMXDI, CMXHT, CMXCVD, CMXCPD, CMXCKD, CMXCLD and RFXONC.\nWe will create a new variable called CMXCOM that quantifies how many comorbidities an individual has. We will have four categories:\n\n0 comorbidities\n1 comorbidities\n2 comorbidities\n3+ comorbidities\n\nThe process to achieve this is not as easy as straightforward as it would be using base R functions on our computer.\nTo begin, we will take a look at how the variables are encoded. Looking at the codebook they should all be dicotomous variables encoded as \"Yes\" / \"No\". We can verify that with the functions we’ve covered at the Part 2 of this workshop.\n\nds.table(\"data$RFXOB\")$output.list$TABLES.COMBINED_all.sources_counts\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\n\n\ndata$RFXOB\n No Yes  NA \n149 101 749 \n\n\nNow, we will recode all the variabled so they are coded as 0 / 1.\n\nvariables <- c(\"RFXOB\", \"CMXDI\", \"CMXHT\", \"CMXCVD\", \n               \"CMXCPD\", \"CMXCKD\", \"CMXCLD\", \"RFXONC\")\n\nfor (x in variables){\n  ds.recodeValues(var.name = paste0(\"data$\", x), \n                  values2replace.vector = c(\"Yes\", \"No\"), \n                  new.values.vector = c(1, 0),\n                  newobj = paste0(x, \"_recoded\"))\n}\n\nWe have created a new object for each comorbiditie. The problem we now have is that the objects are of class character, because the ds.recodeValues function does not change the class.\n\nds.class(\"RFXOB_recoded\")\n\n$study1\n[1] \"character\"\n\n\nSince we want to count the numbers of comorbidities for each individual, it is important that we have a numeric variable. As we have prevously seen we can easily obtain this.\n\nfor (x in variables){\n  ds.asNumeric(x.name = paste0(x, \"_recoded\"), \n               newobj = paste0(x, \"_recoded_num\"))\n}\n\n\nds.class(\"RFXOB_recoded_num\")\n\n$study1\n[1] \"numeric\"\n\n\nNow we have a collection of variables encoded as we want and ready to be combined. First, we will join them on a data.frame.\n\nds.dataFrame(x = paste0(variables, \"_recoded_num\"), \n             newobj = \"joint_comorbidities\")\n\n$is.object.created\n[1] \"A data object <joint_comorbidities> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<joint_comorbidities> appears valid in all sources\"\n\n\nOn the server we have created a table that looks something like this:\n\n\n\n\n\n\n\n\n\n\nid\nRFXOB_recoded_num\nCMXDI_recoded_num\nCMXHT_recoded_num\n…\n\n\n\n\nIndividual 1\n0\n0\n0\n…\n\n\nIndividual 2\n1\n1\n1\n…\n\n\n…\n…\n…\n…\n…\n\n\n\nThe variable we want to create is the amount of comorbidities by individual, therefore we have to perform rowSums.\n\nds.rowColCalc(x = \"joint_comorbidities\", \n              operation = \"rowSums\", \n              newobj = \"new_variable\")\n\nWe are almost done, we will convert our new variable to be a factor.\n\nds.asFactor(input.var.name = \"new_variable\", \n            newobj.name = \"new_variable_factor\")\n\n$all.unique.levels\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\n$return.message\n[1] \"Data object <new_variable_factor> correctly created in all specified data sources\"\n\n\nWe can see that when we call the function ds.asFactor we receive the actual levels of the variable. We have said that the levels we actually want are 0, 1, 2, 3+.\nTo achieve this we just have to re-code the levels.\n\nds.recodeValues(var.name = \"new_variable_factor\", \n                values2replace.vector = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n                new.values.vector = c(\"0\", \"1\", \"2\",\"3+\", \"3+\", \"3+\",\"3+\"), \n                newobj = \"CMXCOM\")\n\n$is.object.created\n[1] \"A data object <CMXCOM> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<CMXCOM> appears valid in all sources\"\n\n\nFinally, we merge the new variable to our original data, and we are finished.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCOM)\")"
  },
  {
    "objectID": "workshop_part4.html#complete-cases",
    "href": "workshop_part4.html#complete-cases",
    "title": "Part 3: Data wranggling",
    "section": "Complete cases",
    "text": "Complete cases\nReal data tends to have missing values, however, some specific tools or functions expect complete data as input. For that reason there is a function specifically for that. Let’s suppose we want to have the complete cases for the variables DMRAGEYR and CMXDI. First we join them on a new data.frame.\n\nds.dataFrame(x = c(\"data$DMRAGEYR\", \"data$CMXDI\"), newobj = \"data_subset\")\n\n$is.object.created\n[1] \"A data object <data_subset> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset> appears valid in all sources\"\n\n\nNow we can get the complete cases.\n\nds.completeCases(x1 = \"data_subset\", newobj = \"data_subset_ccases\")\n\n$is.object.created\n[1] \"A data object <data_subset_ccases> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset_ccases> appears valid in all sources\"\n\n\nWe can check the dimensions before and after.\n\nds.dim(\"data_subset\")\n\n$`dimensions of data_subset in study1`\n[1] 999   2\n\n$`dimensions of data_subset in combined studies`\n[1] 999   2\n\nds.dim(\"data_subset_ccases\")\n\n$`dimensions of data_subset_ccases in study1`\n[1] 249   2\n\n$`dimensions of data_subset_ccases in combined studies`\n[1] 249   2"
  },
  {
    "objectID": "workshop_part4.html#dates-what-to-do",
    "href": "workshop_part4.html#dates-what-to-do",
    "title": "Part 3: Data wranggling",
    "section": "Dates: What to do…",
    "text": "Dates: What to do…\nThe unCoVer consortia have many variables that correspond to dates. At the moment DataSHIELD has no functions to work with dates. We could develop a package to work with them if researchers can benefit from that."
  },
  {
    "objectID": "workshop_part3.html",
    "href": "workshop_part3.html",
    "title": "Part 4: Descriptive analysis",
    "section": "",
    "text": "We have already checked that our data is compliant with the codebook. Now we can proceed with our analysis using the selected variables. To begin, we will perform a brief descriptive analysis, calculating some statistics, tables of contingence and doing some graphical visualizations."
  },
  {
    "objectID": "workshop_part3.html#descriptive-statistics",
    "href": "workshop_part3.html#descriptive-statistics",
    "title": "Part 4: Descriptive analysis",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nThere is a collection of DataSHIELD functions to get the descriptive statistics of a variable. Those are the functions ds.var, ds.mean, ds.table among others. However, there is a function from the dsHelper package that automatically performs all the function calls on the background, being a much more user-friendly alternative. The only downside is that it requires the categorical variables to be factors, we can easily create a new factor variable and add it as a new column to our data.\n\nds.asFactor(input.var.name = \"data$SMXAPA\", newobj.name = \"SMXAPA_factor\")\n\n$all.unique.levels\n[1] \"No\"  \"Yes\"\n\n$return.message\n[1] \"Data object <SMXAPA_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, SMXAPA_factor)\")\n\ndh.getStats(df = \"data\", vars = c(\"DMRAGEYR\", \"SMXAPA_factor\"))\n\n$categorical\n# A tibble: 4 x 10\n  variable      cohort   category value cohort_n valid_n missing_n perc_valid\n  <chr>         <chr>    <fct>    <int>    <int>   <int>     <int>      <dbl>\n1 SMXAPA_factor combined No         236      999     252       747      93.6 \n2 SMXAPA_factor combined Yes         16      999     252       747       6.35\n3 SMXAPA_factor study1   No         236      999     252       747      93.6 \n4 SMXAPA_factor study1   Yes         16      999     252       747       6.35\n# ... with 2 more variables: perc_missing <dbl>, perc_total <dbl>\n\n$continuous\n# A tibble: 2 x 15\n  variable cohort    mean std.dev perc_5 perc_10 perc_25 perc_50 perc_75 perc_90\n  <chr>    <chr>    <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 DMRAGEYR study1    63.8    13.0   40.4    46.9    54.2      65      72    80.1\n2 DMRAGEYR combined  63.8    13.0   40.4    46.9    54.2      65      72    80.1\n# ... with 5 more variables: perc_95 <dbl>, valid_n <dbl>, cohort_n <dbl>,\n#   missing_n <dbl>, missing_perc <dbl>"
  },
  {
    "objectID": "workshop_part3.html#graphical-visualizations",
    "href": "workshop_part3.html#graphical-visualizations",
    "title": "Part 4: Descriptive analysis",
    "section": "Graphical visualizations",
    "text": "Graphical visualizations\nThere is a rich collection of functions to perform graphical visualization of the variables.\n\nBoxplot\nThe boxplot is one of the newest plots available on DataSHIELD. It uses the ggplot2 library, providing lots of customization options.\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\")\n\n\n\n\nThere are grouping options on the boxplot function which are very useful to have greater insights of the data. We can group using factor variables, so in this example we will use the previously created SMXAPA_factor variable.\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\", group = \"SMXAPA_factor\")\n\n\n\n\nWe could even perform a second grouping using the argument group2 and stating another factor variable.\n\n\nHistogram\nTo have an idea of the distribution of a variable we can use histograms.\n\nds.histogram(\"data$DMRAGEYR\")\n\nWarning: study1: 1 invalid cells\n\n\n\n\n\n$breaks\n [1] 26.35662 33.27495 40.19328 47.11160 54.02993 60.94826 67.86658 74.78491\n [9] 81.70324 88.62156 95.53989\n\n$counts\n [1]  0 11 14 36 29 54 51 31 17  5\n\n$density\n [1] 0.000000000 0.006359919 0.008094443 0.020814282 0.016767060 0.031221422\n [7] 0.029486899 0.017923409 0.009828966 0.002890872\n\n$mids\n [1] 29.81579 36.73411 43.65244 50.57077 57.48909 64.40742 71.32575 78.24407\n [9] 85.16240 92.08073\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\n\n\nScatter plot\nDataSHIELD has a scatter plot functionality that outputs noise-affected data points, depending on the security configuration of the Opal, the noise levels can make this plot to be hugely distorted.\n\nds.scatterPlot(x = \"data$DMRAGEYR\", y = \"data$LBXSC3SIHn\", datasources = connections)\n\n\n\n\n[1] \"Split plot created\"\n\n\n\n\nHeatmap plot\nIn a similar fashion than the scatter plot, we can visualize a two dimensional distribution of the points but in this case by density.\n\nds.heatmapPlot(x = \"data$DMRAGEYR\", y = \"data$LBXSC3SIHn\")\n\nstudy1: Number of invalid cells (cells with counts >0 and < nfilter.tab ) is 73"
  },
  {
    "objectID": "workshop_part3.html#dimensional-contingency-table",
    "href": "workshop_part3.html#dimensional-contingency-table",
    "title": "Part 4: Descriptive analysis",
    "section": "2-Dimensional contingency table",
    "text": "2-Dimensional contingency table\nOn the previous part, we have already seen that DataSHIELD has a function to calculate uni-dimensional contingency tables. This same function, can also be used for bi-dimensional contingency tables. It is important to note that this function is a little bit tricky sometimes, as it is quite common that the 2D contingency table has disclosive outputs, therefore we just get an error message.\n\nds.table(\"data$DMRAGEYR\", \"data$LBXSC3SIHn\")\n\n\n All studies failed for reasons identified below \n\n\nStudy 1 :  Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3 \n\n\n$validity.message\n[1] \"All studies failed for reasons identified below\"\n\n$error.messages\n$error.messages$study1\n[1] \"Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3\"\n\n\nThere are other variables that do produce valid non-disclosive results.\n\nctable <- ds.table(\"data$SMXCPA\", \"data$SMXSLA\")$output.list$TABLES.COMBINED_all.sources_counts\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\n\nctable\n\n           data$SMXSLA\ndata$SMXCPA  No Yes  NA\n        No  202  21   0\n        Yes  26   3   0\n        NA    0   0 747\n\n\n\nFisher test\nGiven the calculated contingency table, we are now in the position to perform a Fisher’s exact test.\n\nstats::fisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "href": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "title": "Part 4: Descriptive analysis",
    "section": "Statistics of a continuous variable grouped by a categorical variable",
    "text": "Statistics of a continuous variable grouped by a categorical variable\nWe may have interest on knowing the value of certain statistics of a variable when grouping with a category. As an example we will calculate the mean of the variable DMRAGEYR (Age) grouped by the variable CMXDI (Diabetes). Please note this is just an example, those two variables are not expected to be related.\nTo compute this statistic it is important that we have the categorical variable as a factor, otherwise the grouped statistic calculation will fail. Also, we have to create separate objects for the two variables, we can’t merge them into a data.frame and call them using the typical formulation data.frame$variable, for some reason it fails (to be reported as a bug to the core DataSHIELD team).\n\nds.assign(toAssign = \"data$DMRAGEYR\",\n            newobj =  \"DMRAGEYR\")\nds.assign(toAssign = \"data$CMXDI\",\n            newobj =  \"CMXDI\")\nds.asFactor(input.var.name = \"CMXDI\", newobj.name = \"CMXDI_factor\")\n\n$all.unique.levels\n[1] \"No\"  \"Yes\"\n\n$return.message\n[1] \"Data object <CMXDI_factor> correctly created in all specified data sources\"\n\nds.tapply(X.name = \"DMRAGEYR\", INDEX.names = \"CMXDI_factor\", FUN.name = \"mean\")\n\n$study1\n$study1$Mean\n CMXDI_factor.No CMXDI_factor.Yes \n        63.32787         65.54545 \n\n$study1$N\n CMXDI_factor.No CMXDI_factor.Yes \n             183               66 \n\n\nThis function can take other expressions on the argument FUN.name, more precisely:\n\n\"mean\"\n\"sd\"\n\"sum\"\n\"quantile\""
  },
  {
    "objectID": "workshop_part3.html#student-t-test",
    "href": "workshop_part3.html#student-t-test",
    "title": "Part 4: Descriptive analysis",
    "section": "Student t-test",
    "text": "Student t-test\nWe have two options to perform a t-test, and our choice will depend on whether we want to use pooled methods or we want to calculate the t-test on a single study server.\n\nPooled t-test\nTo perform a pooled t-test, we will use a collection of DataSHIELD functions to calculate the statistic.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWhere:\n\n\\(\\bar{x}\\) is the observed mean of the sample.\n\\(s\\) is the standard devuation of the sample.\n\\(n\\) is the sample size.\n\nWe will use the following code to perform this operations:\n\nmean1 <- ds.mean(\"data$DMRAGEYR\")$Mean.by.Study[1]\nmean2 <- ds.mean(\"data$LBXSC3SIHn\")$Mean.by.Study[1]\nsd1 <- sqrt(ds.var(\"data$DMRAGEYR\")$Variance.by.Study[1])\nsd2 <- sqrt(ds.var(\"data$LBXSC3SIHn\")$Variance.by.Study[1])\nn1 <- ds.length(\"data$DMRAGEYR\")[[1]] - ds.numNA(\"data$DMRAGEYR\")[[1]]\nn2 <- ds.length(\"data$LBXSC3SIHn\")[[1]] - ds.numNA(\"data$LBXSC3SIHn\")[[1]]\n\n(mean1 - mean2) / sqrt(sd1^2/n1 + sd2^2/n2)\n\n[1] 46.00566\n\n\nYou can expect to see a wrapper for this code on a future dsHelper release (dsHelper::dh.ttest('sample1', 'sample2').\n\n\nSingle study t-test\nIf we are not interested on using pooled functionalities, it is much simpler to perform a t-test.\n\nDSI::datashield.aggregate(connections, \"t.test(data$DMRAGEYR, data$LBXSC3SIHn)\")\n\n$study1\n\n    Welch Two Sample t-test\n\ndata:  data$DMRAGEYR and data$LBXSC3SIHn\nt = 46.006, df = 312.25, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 38.5088 41.9499\nsample estimates:\nmean of x mean of y \n 63.83200  23.60265"
  },
  {
    "objectID": "workshop_part3.html#analysis-of-variance-anova",
    "href": "workshop_part3.html#analysis-of-variance-anova",
    "title": "Part 4: Descriptive analysis",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\nA student is developing a DataSHIELD function to perform ANOVAs. You can expect it to be available in the following months, probably as part of the dsML package."
  },
  {
    "objectID": "workshop_part5.html",
    "href": "workshop_part5.html",
    "title": "Part 5: Statistical models",
    "section": "",
    "text": "Now that we have covered the basics, we can begin to analyze the data in more depth. In this section we will go over some of the statistical models we can fit by using DataSHIELD."
  },
  {
    "objectID": "workshop_part5.html#logistical-regression",
    "href": "workshop_part5.html#logistical-regression",
    "title": "Part 5: Statistical models",
    "section": "Logistical regression",
    "text": "Logistical regression\nWe will perform some logistic regressions to see the association of different variables to the survival condition (DSXOS variable). To achieve this, we will have to apply some of the concepts we have already seen in order to prepare our data.\n\nPreparing the data\nFor the logistic regression, the variables we will use are the following:\n\nDSXOS: Character variable. Outcome status encoded as Deceased, Recovered and Transferred. To perform a logistic regression this variable will have to be re-encoded as 1/0 (case/control); the case being Deceased and the controls being Recovered and Transferred.\nCMXCOM: Factor variable. Number of comorbidities, encoded as 0, 1, 2 and 3+.\nCMXCLD: Character variable. Chronic liver disease, encoded as Yes and No.\nRFXONC: Character variable. Oncology, encoded as Yes and No.\n\nWe only have to do some data wrangling for the DSXOS variable, first we do the re-coding.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = c(\"Deceased\", \"Recovered\", \"Transferred\"), \n                new.values.vector = c(1, 0, 0),\n                newobj = \"DSXOS_recoded\")\n\nError: There are some DataSHIELD errors, list them with datashield.errors()\n\ndatashield.errors()\n\n$study1\n[1] \"Command 'recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\n    \\\"1,0,0\\\", NULL)' failed on 'study1': Error while evaluating 'is.null(base::assign('DSXOS_recoded', value={dsBase::recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\\"1,0,0\\\", NULL)}))' -> Error : Error: values2replace.text argument too long (see nfilter.stringShort)\\n\"\n\n\nWe can see that the DataSHIELD filter is complaining that the length of the values2replace.vector is too large. To overcome this issue, we can perform separate function calls for each of the levels.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = \"Deceased\", \n                new.values.vector = 1,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Recovered\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Transferred\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\n\nBy doing that we successfully created the object DSXOS_recoded, now we just have to convert it to numerical.\n\n# ds.table does not work, I am not sure why\nds.table1D(\"DSXOS_recoded\")\n\nWarning: 'ds.table1D' is deprecated.\nUse 'ds.table' instead.\nSee help(\"Deprecated\")\n\n\n$counts\n      DSXOS_recoded\n''              746\n0               220\n1                33\nTotal           999\n\n$percentages\n      DSXOS_recoded\n''            74.67\n0             22.02\n1              3.30\nTotal        100.00\n\n$validity\n[1] \"All tables are valid!\"\n\n\n\nds.asNumeric(x.name = \"DSXOS_recoded\", newobj = \"DSXOS_recoded_num\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded_num> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded_num> appears valid in all sources\"\n\n\nFinally, we add it to our data.frame with the other covariates, and we are ready to perform the logistic regression.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, DSXOS_recoded_num)\")\n\n\n\nFitting the models\n\nPooled analysis\nNow we can fit the models. First, we will calculate the association of the outcome status to the covariables chronic liver disease and oncology.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCLD + RFXONC\", data = \"data\", family = \"binomial\")\n\nIteration 1...\n\n\nCURRENT DEVIANCE:      349.346179002212\n\n\nIteration 2...\n\n\nCURRENT DEVIANCE:      197.274454485614\n\n\nIteration 3...\n\n\nCURRENT DEVIANCE:      190.668047341288\n\n\nIteration 4...\n\n\nCURRENT DEVIANCE:      190.517887214012\n\n\nIteration 5...\n\n\nCURRENT DEVIANCE:      190.517748305142\n\n\nIteration 6...\n\n\nCURRENT DEVIANCE:      190.517748305008\n\n\nSUMMARY OF MODEL STATE after iteration 6\n\n\nCurrent deviance 190.517748305008 on 249 degrees of freedom\n\n\nConvergence criterion TRUE (7.05556173129648e-13)\n\n\n\nbeta: -2.07436706515554 0.481163947699337 1.34712733484881\n\n\n\nInformation matrix overall:\n\n\n            (Intercept) CMXCLDYes RFXONCYes\n(Intercept)   27.934795 4.0369214 3.3214179\nCMXCLDYes      4.036921 4.0369214 0.2462533\nRFXONCYes      3.321418 0.2462533 3.3214179\n\n\n\nScore vector overall:\n\n\n                     [,1]\n(Intercept) -5.176640e-11\nCMXCLDYes   -1.031952e-13\nRFXONCYes   -6.572520e-14\n\n\n\nCurrent deviance: 190.517748305008\n\n\n$Nvalid\n[1] 252\n\n$Nmissing\n[1] 747\n\n$Ntotal\n[1] 999\n\n$disclosure.risk\n       RISK OF DISCLOSURE\nstudy1                  0\n\n$errorMessage\n       ERROR MESSAGES\nstudy1 \"No errors\"   \n\n$nsubs\n[1] 252\n\n$iter\n[1] 6\n\n$family\n\nFamily: binomial \nLink function: logit \n\n\n$formula\n[1] \"DSXOS_recoded_num ~ CMXCLD + RFXONC\"\n\n$coefficients\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -2.0743671  0.2180219 -9.5144888 1.826085e-21   -2.5016822\nCMXCLDYes    0.4811639  0.5395664  0.8917604 3.725214e-01   -0.5763668\nRFXONCYes    1.3471273  0.5861413  2.2982979 2.154484e-02    0.1983115\n            high0.95CI.LP      P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)     -1.647052 0.1116133     0.07574034       0.1615078\nCMXCLDYes        1.538695 1.6179565     0.56193629       4.6585055\nRFXONCYes        2.495943 3.8463603     1.21934216      12.1331717\n\n$dev\n[1] 190.5177\n\n$df\n[1] 249\n\n$output.information\n[1] \"SEE TOP OF OUTPUT FOR INFORMATION ON MISSING DATA AND ERROR MESSAGES\"\n\n\nAnd we also calculate the association of the outcome status to the number of comorbidities.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCOM\", data = \"data\", family = \"binomial\")\n\nIteration 1...\n\n\nCURRENT DEVIANCE:      350.732473363332\n\n\nIteration 2...\n\n\nCURRENT DEVIANCE:      198.483331037372\n\n\nIteration 3...\n\n\nCURRENT DEVIANCE:      191.27692242333\n\n\nIteration 4...\n\n\nCURRENT DEVIANCE:      190.960473189413\n\n\nIteration 5...\n\n\nCURRENT DEVIANCE:      190.957960191484\n\n\nIteration 6...\n\n\nCURRENT DEVIANCE:      190.957959832379\n\n\nSUMMARY OF MODEL STATE after iteration 6\n\n\nCurrent deviance 190.957959832379 on 249 degrees of freedom\n\n\nConvergence criterion TRUE (1.87955786013388e-09)\n\n\n\nbeta: -2.68557734525015 0.813775168348555 0.451985123743052 1.18149994847388\n\n\n\nInformation matrix overall:\n\n\n            (Intercept) CMXCOM1  CMXCOM2 CMXCOM3+\n(Intercept)   28.155138     5.2 5.419355 14.72727\nCMXCOM1        5.200000     5.2 0.000000  0.00000\nCMXCOM2        5.419355     0.0 5.419355  0.00000\nCMXCOM3+      14.727273     0.0 0.000000 14.72727\n\n\n\nScore vector overall:\n\n\n                     [,1]\n(Intercept) -1.565667e-07\nCMXCOM1     -1.960654e-13\nCMXCOM2     -3.247187e-10\nCMXCOM3+    -6.666889e-14\n\n\n\nCurrent deviance: 190.957959832379\n\n\n$Nvalid\n[1] 253\n\n$Nmissing\n[1] 746\n\n$Ntotal\n[1] 999\n\n$disclosure.risk\n       RISK OF DISCLOSURE\nstudy1                  0\n\n$errorMessage\n       ERROR MESSAGES\nstudy1 \"No errors\"   \n\n$nsubs\n[1] 253\n\n$iter\n[1] 6\n\n$family\n\nFamily: binomial \nLink function: logit \n\n\n$formula\n[1] \"DSXOS_recoded_num ~ CMXCOM\"\n\n$coefficients\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -2.6855773  0.5967081 -4.5006549 6.774442e-06  -3.85510378\nCMXCOM1      0.8137752  0.7405189  1.0989255 2.718005e-01  -0.63761526\nCMXCOM2      0.4519851  0.7352444  0.6147413 5.387256e-01  -0.98906751\nCMXCOM3+     1.1814999  0.6511235  1.8145558 6.959220e-02  -0.09467868\n            high0.95CI.LP       P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)     -1.516051 0.06382979     0.02073247       0.1800438\nCMXCOM1          2.265166 2.25641026     0.52855138       9.6327196\nCMXCOM2          1.893038 1.57142857     0.37192334       6.6395073\nCMXCOM3+         2.457679 3.25925926     0.90966518      11.6776712\n\n$dev\n[1] 190.958\n\n$df\n[1] 249\n\n$output.information\n[1] \"SEE TOP OF OUTPUT FOR INFORMATION ON MISSING DATA AND ERROR MESSAGES\"\n\n\n\n\nMeta-analysis\nTo fit the same models to be meta-analyzed, we just have to use a different function with the same structure.\n\nds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCLD + RFXONC\", dataName = \"data\", family = \"binomial\")\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\n\n$output.summary\n$output.summary$study1\n$output.summary$study1$rank\n[1] 3\n\n$output.summary$study1$aic\n[1] 196.5177\n\n$output.summary$study1$iter\n[1] 4\n\n$output.summary$study1$converged\n[1] TRUE\n\n$output.summary$study1$boundary\n[1] FALSE\n\n$output.summary$study1$na.action\n$output.summary$study1$na.action$na.action\n[1] \"na.omit\"\n\n\n$output.summary$study1$call\nglm(formula = formula, family = binomial, data = data, x = TRUE)\n\n$output.summary$study1$terms\nDSXOS_recoded_num ~ CMXCLD + RFXONC\nattr(,\"variables\")\nlist(DSXOS_recoded_num, CMXCLD, RFXONC)\nattr(,\"factors\")\n                  CMXCLD RFXONC\nDSXOS_recoded_num      0      0\nCMXCLD                 1      0\nRFXONC                 0      1\nattr(,\"term.labels\")\n[1] \"CMXCLD\" \"RFXONC\"\nattr(,\"order\")\n[1] 1 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n<environment: R_GlobalEnv>\nattr(,\"predvars\")\nlist(DSXOS_recoded_num, CMXCLD, RFXONC)\nattr(,\"dataClasses\")\nDSXOS_recoded_num            CMXCLD            RFXONC \n        \"numeric\"       \"character\"       \"character\" \n\n$output.summary$study1$contrasts\n$output.summary$study1$contrasts$CMXCLD\n[1] \"contr.treatment\"\n\n$output.summary$study1$contrasts$RFXONC\n[1] \"contr.treatment\"\n\n\n$output.summary$study1$aliased\n(Intercept)   CMXCLDYes   RFXONCYes \n      FALSE       FALSE       FALSE \n\n$output.summary$study1$dispersion\n[1] 1\n\n$output.summary$study1$data\n[1] \"data\"\n\n$output.summary$study1$df\n[1]   3 249   3\n\n$output.summary$study1$Ntotal\n[1] 999\n\n$output.summary$study1$Nvalid\n[1] 252\n\n$output.summary$study1$Nmissing\n[1] 747\n\n$output.summary$study1$cov.unscaled\n            (Intercept)  CMXCLDYes   RFXONCYes\n(Intercept)  0.04753155 -0.0448349 -0.04420746\nCMXCLDYes   -0.04483490  0.2911297  0.02325040\nRFXONCYes   -0.04420746  0.0232504  0.34355945\n\n$output.summary$study1$cov.scaled\n            (Intercept)  CMXCLDYes   RFXONCYes\n(Intercept)  0.04753155 -0.0448349 -0.04420746\nCMXCLDYes   -0.04483490  0.2911297  0.02325040\nRFXONCYes   -0.04420746  0.0232504  0.34355945\n\n$output.summary$study1$offset\nNULL\n\n$output.summary$study1$weights\nNULL\n\n$output.summary$study1$VarCovMatrix\n            (Intercept)  CMXCLDYes   RFXONCYes\n(Intercept)  0.04753155 -0.0448349 -0.04420746\nCMXCLDYes   -0.04483490  0.2911297  0.02325040\nRFXONCYes   -0.04420746  0.0232504  0.34355945\n\n$output.summary$study1$CorrMatrix\n           [,1]        [,2]        [,3]\n[1,]  1.0000000 -0.38113777 -0.34594232\n[2,] -0.3811378  1.00000000  0.07351675\n[3,] -0.3459423  0.07351675  1.00000000\n\n$output.summary$study1$deviance.null\n[1] 195.6493\n\n$output.summary$study1$df.null\n[1] 251\n\n$output.summary$study1$deviance.resid\n[1] 190.5177\n\n$output.summary$study1$df.resid\n[1] 249\n\n$output.summary$study1$formula\nDSXOS_recoded_num ~ CMXCLD + RFXONC\n\n$output.summary$study1$family\n\nFamily: binomial \nLink function: logit \n\n\n$output.summary$study1$coefficients\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -2.0743671  0.2180173 -9.5146899 1.822557e-21\nCMXCLDYes    0.4811639  0.5395643  0.8917638 3.725195e-01\nRFXONCYes    1.3471273  0.5861394  2.2983052 2.154443e-02\n\n\n$output.summary$input.beta.matrix.for.SLMA\n            betas study 1\n(Intercept)    -2.0743671\nCMXCLDYes       0.4811639\nRFXONCYes       1.3471273\n\n$output.summary$input.se.matrix.for.SLMA\n            ses study 1\n(Intercept)   0.2180173\nCMXCLDYes     0.5395643\nRFXONCYes     0.5861394\n\n\n$num.valid.studies\n[1] 1\n\n$betamatrix.all\n            betas study 1\n(Intercept)    -2.0743671\nCMXCLDYes       0.4811639\nRFXONCYes       1.3471273\n\n$sematrix.all\n            ses study 1\n(Intercept)   0.2180173\nCMXCLDYes     0.5395643\nRFXONCYes     0.5861394\n\n$betamatrix.valid\n(Intercept)   CMXCLDYes   RFXONCYes \n -2.0743671   0.4811639   1.3471273 \n\n$sematrix.valid\n(Intercept)   CMXCLDYes   RFXONCYes \n  0.2180173   0.5395643   0.5861394 \n\n$SLMA.pooled.ests.matrix\n      pooled.ML     se.ML pooled.REML   se.REML  pooled.FE     se.FE\n[1,] -2.0743671 0.2180173  -2.0743671 0.2180173 -2.0743671 0.2180173\n[2,]  0.4811639 0.5395643   0.4811639 0.5395643  0.4811639 0.5395643\n[3,]  1.3471273 0.5861394   1.3471273 0.5861394  1.3471273 0.5861394\n\n$is.object.created\n[1] \"A data object <new.glm.obj> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<new.glm.obj> appears valid in all sources\"\n\n\n\nds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCOM\", dataName = \"data\", family = \"binomial\")\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\n\n$output.summary\n$output.summary$study1\n$output.summary$study1$rank\n[1] 4\n\n$output.summary$study1$aic\n[1] 198.958\n\n$output.summary$study1$iter\n[1] 5\n\n$output.summary$study1$converged\n[1] TRUE\n\n$output.summary$study1$boundary\n[1] FALSE\n\n$output.summary$study1$na.action\n$output.summary$study1$na.action$na.action\n[1] \"na.omit\"\n\n\n$output.summary$study1$call\nglm(formula = formula, family = binomial, data = data, x = TRUE)\n\n$output.summary$study1$terms\nDSXOS_recoded_num ~ CMXCOM\nattr(,\"variables\")\nlist(DSXOS_recoded_num, CMXCOM)\nattr(,\"factors\")\n                  CMXCOM\nDSXOS_recoded_num      0\nCMXCOM                 1\nattr(,\"term.labels\")\n[1] \"CMXCOM\"\nattr(,\"order\")\n[1] 1\nattr(,\"intercept\")\n[1] 1\nattr(,\"response\")\n[1] 1\nattr(,\".Environment\")\n<environment: R_GlobalEnv>\nattr(,\"predvars\")\nlist(DSXOS_recoded_num, CMXCOM)\nattr(,\"dataClasses\")\nDSXOS_recoded_num            CMXCOM \n        \"numeric\"          \"factor\" \n\n$output.summary$study1$contrasts\n$output.summary$study1$contrasts$CMXCOM\n[1] \"contr.treatment\"\n\n\n$output.summary$study1$aliased\n(Intercept)     CMXCOM1     CMXCOM2    CMXCOM3+ \n      FALSE       FALSE       FALSE       FALSE \n\n$output.summary$study1$dispersion\n[1] 1\n\n$output.summary$study1$data\n[1] \"data\"\n\n$output.summary$study1$df\n[1]   4 249   4\n\n$output.summary$study1$Ntotal\n[1] 999\n\n$output.summary$study1$Nvalid\n[1] 253\n\n$output.summary$study1$Nmissing\n[1] 746\n\n$output.summary$study1$cov.unscaled\n            (Intercept)    CMXCOM1    CMXCOM2   CMXCOM3+\n(Intercept)   0.3560569 -0.3560569 -0.3560569 -0.3560569\nCMXCOM1      -0.3560569  0.5483646  0.3560569  0.3560569\nCMXCOM2      -0.3560569  0.3560569  0.5405807  0.3560569\nCMXCOM3+     -0.3560569  0.3560569  0.3560569  0.4239581\n\n$output.summary$study1$cov.scaled\n            (Intercept)    CMXCOM1    CMXCOM2   CMXCOM3+\n(Intercept)   0.3560569 -0.3560569 -0.3560569 -0.3560569\nCMXCOM1      -0.3560569  0.5483646  0.3560569  0.3560569\nCMXCOM2      -0.3560569  0.3560569  0.5405807  0.3560569\nCMXCOM3+     -0.3560569  0.3560569  0.3560569  0.4239581\n\n$output.summary$study1$offset\nNULL\n\n$output.summary$study1$weights\nNULL\n\n$output.summary$study1$VarCovMatrix\n            (Intercept)    CMXCOM1    CMXCOM2   CMXCOM3+\n(Intercept)   0.3560569 -0.3560569 -0.3560569 -0.3560569\nCMXCOM1      -0.3560569  0.5483646  0.3560569  0.3560569\nCMXCOM2      -0.3560569  0.3560569  0.5405807  0.3560569\nCMXCOM3+     -0.3560569  0.3560569  0.3560569  0.4239581\n\n$output.summary$study1$CorrMatrix\n           [,1]       [,2]       [,3]       [,4]\n[1,]  1.0000000 -0.8057958 -0.8115764 -0.9164277\n[2,] -0.8057958  1.0000000  0.6539649  0.7384536\n[3,] -0.8115764  0.6539649  1.0000000  0.7437511\n[4,] -0.9164277  0.7384536  0.7437511  1.0000000\n\n$output.summary$study1$deviance.null\n[1] 195.9295\n\n$output.summary$study1$df.null\n[1] 252\n\n$output.summary$study1$deviance.resid\n[1] 190.958\n\n$output.summary$study1$df.resid\n[1] 249\n\n$output.summary$study1$formula\nDSXOS_recoded_num ~ CMXCOM\n\n$output.summary$study1$family\n\nFamily: binomial \nLink function: logit \n\n\n$output.summary$study1$coefficients\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -2.6855773  0.5967050 -4.5006784 6.773694e-06\nCMXCOM1      0.8137752  0.7405164  1.0989293 2.717989e-01\nCMXCOM2      0.4519851  0.7352419  0.6147434 5.387242e-01\nCMXCOM3+     1.1814999  0.6511207  1.8145638 6.959098e-02\n\n\n$output.summary$input.beta.matrix.for.SLMA\n            betas study 1\n(Intercept)    -2.6855773\nCMXCOM1         0.8137752\nCMXCOM2         0.4519851\nCMXCOM3+        1.1814999\n\n$output.summary$input.se.matrix.for.SLMA\n            ses study 1\n(Intercept)   0.5967050\nCMXCOM1       0.7405164\nCMXCOM2       0.7352419\nCMXCOM3+      0.6511207\n\n\n$num.valid.studies\n[1] 1\n\n$betamatrix.all\n            betas study 1\n(Intercept)    -2.6855773\nCMXCOM1         0.8137752\nCMXCOM2         0.4519851\nCMXCOM3+        1.1814999\n\n$sematrix.all\n            ses study 1\n(Intercept)   0.5967050\nCMXCOM1       0.7405164\nCMXCOM2       0.7352419\nCMXCOM3+      0.6511207\n\n$betamatrix.valid\n(Intercept)     CMXCOM1     CMXCOM2    CMXCOM3+ \n -2.6855773   0.8137752   0.4519851   1.1814999 \n\n$sematrix.valid\n(Intercept)     CMXCOM1     CMXCOM2    CMXCOM3+ \n  0.5967050   0.7405164   0.7352419   0.6511207 \n\n$SLMA.pooled.ests.matrix\n      pooled.ML     se.ML pooled.REML   se.REML  pooled.FE     se.FE\n[1,] -2.6855773 0.5967050  -2.6855773 0.5967050 -2.6855773 0.5967050\n[2,]  0.8137752 0.7405164   0.8137752 0.7405164  0.8137752 0.7405164\n[3,]  0.4519851 0.7352419   0.4519851 0.7352419  0.4519851 0.7352419\n[4,]  1.1814999 0.6511207   1.1814999 0.6511207  1.1814999 0.6511207\n\n$is.object.created\n[1] \"A data object <new.glm.obj> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<new.glm.obj> appears valid in all sources\"\n\n\nAt this moment, the consortia is using dsBaseClient 6.1.1. In the new version 6.2, there is a function to visualize the meta-analyzed coefficients using forestplots."
  },
  {
    "objectID": "workshop_part5.html#piecewise-exponential-regression",
    "href": "workshop_part5.html#piecewise-exponential-regression",
    "title": "Part 5: Statistical models",
    "section": "Piecewise Exponential Regression",
    "text": "Piecewise Exponential Regression\nThe same functions we just used to fit logistical regressions, can also fit piecewise exponential regressions. This is achieved by selecting the output family to be of type poisson. For the variable DATLGT (Length of stay in hospital), we can check that it does follow a poisson distribution.\n\nds.histogram(\"data$DATLGT\")\n\nWarning: study1: 2 invalid cells\n\n\n\n\n\n$breaks\n [1]  0.9761713  7.5248075 14.0734436 20.6220798 27.1707160 33.7193522\n [7] 40.2679884 46.8166246 53.3652608 59.9138970 66.4625332\n\n$counts\n [1]  70 133  38   4   5   0   0   0   0   0\n\n$density\n [1] 0.042417653 0.080593541 0.023026726 0.002423866 0.003029832 0.000000000\n [7] 0.000000000 0.000000000 0.000000000 0.000000000\n\n$mids\n [1]  4.250489 10.799126 17.347762 23.896398 30.445034 36.993670 43.542307\n [8] 50.090943 56.639579 63.188215\n\n$xname\n[1] \"xvect\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nAnd we can fit the model.\n\nds.glm(formula = \"DATLGT ~ CMXCOM\", data = \"data\", family = \"poisson\")\n\nIteration 1...\n\n\nCURRENT DEVIANCE:      9031.23112933584\n\n\nIteration 2...\n\n\nCURRENT DEVIANCE:      13445721.9254986\n\n\nIteration 3...\n\n\nCURRENT DEVIANCE:      4924249.82823331\n\n\nIteration 4...\n\n\nCURRENT DEVIANCE:      1792866.24106897\n\n\nIteration 5...\n\n\nCURRENT DEVIANCE:      644372.85806603\n\n\nIteration 6...\n\n\nCURRENT DEVIANCE:      225310.43667617\n\n\nIteration 7...\n\n\nCURRENT DEVIANCE:      74503.0617967646\n\n\nIteration 8...\n\n\nCURRENT DEVIANCE:      22166.8442228996\n\n\nIteration 9...\n\n\nCURRENT DEVIANCE:      5600.46871888188\n\n\nIteration 10...\n\n\nCURRENT DEVIANCE:      1421.16414491348\n\n\nIteration 11...\n\n\nCURRENT DEVIANCE:      812.345972414677\n\n\nIteration 12...\n\n\nCURRENT DEVIANCE:      786.436487861179\n\n\nIteration 13...\n\n\nCURRENT DEVIANCE:      786.356159963903\n\n\nIteration 14...\n\n\nCURRENT DEVIANCE:      786.35615899879\n\n\nSUMMARY OF MODEL STATE after iteration 14\n\n\nCurrent deviance 786.35615899879 on 248 degrees of freedom\n\n\nConvergence criterion TRUE (1.22716669308741e-09)\n\n\n\nbeta: 2.23889727373679 0.204415478752375 0.158997999061583 0.202562056292276\n\n\n\nInformation matrix overall:\n\n\n            (Intercept) CMXCOM1 CMXCOM2 CMXCOM3+\n(Intercept)        2767     518     682     1126\nCMXCOM1             518     518       0        0\nCMXCOM2             682       0     682        0\nCMXCOM3+           1126       0       0     1126\n\n\n\nScore vector overall:\n\n\n                     [,1]\n(Intercept) -4.825297e-07\nCMXCOM1     -1.853942e-07\nCMXCOM2     -4.250067e-11\nCMXCOM3+    -2.970964e-07\n\n\n\nCurrent deviance: 786.35615899879\n\n\n$Nvalid\n[1] 252\n\n$Nmissing\n[1] 747\n\n$Ntotal\n[1] 999\n\n$disclosure.risk\n       RISK OF DISCLOSURE\nstudy1                  0\n\n$errorMessage\n       ERROR MESSAGES\nstudy1 \"No errors\"   \n\n$nsubs\n[1] 252\n\n$iter\n[1] 14\n\n$family\n\nFamily: poisson \nLink function: log \n\n\n$formula\n[1] \"DATLGT ~ CMXCOM\"\n\n$coefficients\n             Estimate Std. Error   z-value      p-value low0.95CI.LP\n(Intercept) 2.2388973 0.04761905 47.016843 0.0000000000   2.14556566\nCMXCOM1     0.2044155 0.06479256  3.154922 0.0016054093   0.07742440\nCMXCOM2     0.1589980 0.06110523  2.602036 0.0092672185   0.03923395\nCMXCOM3+    0.2025621 0.05617538  3.605887 0.0003110887   0.09246034\n            high0.95CI.LP EXPONENTIATED RR low0.95CI.EXP high0.95CI.EXP\n(Intercept)     2.3322289         9.382979      8.546874      10.300876\nCMXCOM1         0.3314066         1.226808      1.080501       1.392926\nCMXCOM2         0.2787621         1.172336      1.040014       1.321493\nCMXCOM3+        0.3126638         1.224536      1.096870       1.367062\n\n$dev\n[1] 786.3562\n\n$df\n[1] 248\n\n$output.information\n[1] \"SEE TOP OF OUTPUT FOR INFORMATION ON MISSING DATA AND ERROR MESSAGES\""
  }
]