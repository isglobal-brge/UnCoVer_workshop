[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataSHIELD Workshop",
    "section": "",
    "text": "This website has been created to host the materials and exercises for the ‘Utilization of the unCoVer toolbox for covidー19 data analysis’, hosted at the UPM-Montegancedo Campus the 6th of May, 2022.\nOn it you will find reading materials, setup tutorials, the workshop indications and practical exercises.\n\n\nBefore the workshop we suggest the atendants to take a look at the “Environment setup” and “Get up to speed” sections. This way they will have their computers with the right software installed to follow the workshop.\nThe password of the study servers are not displayed on this website, they will be provided on-site the workshop day, as well as the required information to connect to the VPN.\nTo help the assistants, we have also prepared a virtual RStudio on the UPM network. This RStudio will have all the required packages, login credentials and URL will also be provided on-site the workshop day. If you want to locally reproduce this RStudio, we have uploaded the Docker image to DockerHub.\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n11:00\nWelcome\n\n\n11:05\nunCoVer toolbox: presentation of the infrastructure\n\n\n11:45\nQ&A\n\n\n12:00\nCase Study (I): Part 0, 1 and 2\n\n\n13:15\nLunch\n\n\n14:15\nCase Study (II): Part 3 and 4\n\n\n15:30\nCoffee break\n\n\n15:45\nCase Study (III): Part 5\n\n\n17:25\nClosing\n\n\n17:30\nEnd\n\n\n\n\n\n\nThis project is funded by the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement No 101016216. Material developed at ISGlobal-Barcelona (BRGE) by Juan R. González and Xavier Escribà Montagut. Special thanks to Augusto Anguita for his help on developing the variable selection script, Soumya Banerjee for his help on the dsSurvival package and Tim Cadman for his help on dsHelper."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Environment setup",
    "section": "",
    "text": "Along this section, we will go over the R packages (and versions) required to navigate through the workshop. We recommend to use RStudio as the integrated development environment (IDE) to use R, although any other IDE can be used. We suggest to use R version > 4.1."
  },
  {
    "objectID": "setup.html#r-packages",
    "href": "setup.html#r-packages",
    "title": "Environment setup",
    "section": "R packages",
    "text": "R packages\nThe required R packages are the following:\n\nopalr 3.1.0: To login to an Opal server using R and retrieve information of the server (projects, resources, tables, etc).\nDSI 1.4.0: The DataSHIELD Interface (DSI) handles the connections to the databases.\nDSOpal 1.3.0: DSOpal is an extension of DSI to connect to to Opal servers.\ndsBaseClient 6.1.1: Implementation of the base R functions (Example: Base package function as.factor is implemented as ds.asFactor).\ndsHelper 0.4.12: dsHelper is a compedium of wrappers for dsBaseClient functions that simplify some common operations to maintain sanity.\ndsSurvivalClient 1.0.0: Client package to perform survival analysis. It requires dsSurvival on the study servers.\ndsMTLClient 0.9: Client package to perform federated multi-task learning. It requires dsMTLBase on the study server.\ndsDatesClient 0.0.0.9000: Client package to work with dates. It requires dsDates on the study server."
  },
  {
    "objectID": "setup.html#install-guide",
    "href": "setup.html#install-guide",
    "title": "Environment setup",
    "section": "Install guide",
    "text": "Install guide\n\n\n\n\n\n\nThe proposed install guide has been tested on a clean installation with R 4.2.0 and RStudio 2022.02.2 Build 485; if any errors occur, please consider using a clean install or refer to the official R documentation regarding the errors you are getting. If you are facing permissions issues, contact your system administrator.\n\n\n\nTo install the required packages we will need the devtools package.\n\ninstall.packages(\"devtools\")\n\nAfterwards, we can install the packages, make sure to have RTools installed:\n\ndevtools::install_version(\"opalr\", version = \"3.1.0\", repos = 'https://cran.r-project.org/')\ndevtools::install_version(\"DSI\", version = \"1.4.0\", repos = 'https://cran.r-project.org/')\ndevtools::install_version(\"DSOpal\", version = \"1.3.0\", repos = 'https://cran.r-project.org/')\ndevtools::install_github(\"datashield/dsBaseClient\", \"6.1.1\")\ndevtools::install_github(\"lifecycle-project/ds-helper\", \"0.4.12\")\ndevtools::install_github(\"neelsoumya/dsSurvivalClient\")\ndevtools::install_github(\"transbioZI/dsMTLClient\")\ndevtools::install_github(\"isglobal-brge/dsDatesClient\")\n\n\n\n\nInstall console"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Get up to speed: Useful reads",
    "section": "",
    "text": "Along this workshop, there are some details regarding DataSHIELD and “resources” that are not explained in detail, it is expected that the reader is familiar with them. If that is not the case, there are other free online books/papers with that knowledge.\n\nDataSHIELD paper: Description of what is DataSHIELD.\nDataSHIELD wiki: Materials about DataSHIELD including:\n\nBeginner material\nRecorded DataSHIELD workshops\nInformation on current release of DataSHIELD\n\nresource book: In this book you will find information about:\n\nDataSHIELD (Section 5)\nWhat are resources (Section 6/7)"
  },
  {
    "objectID": "help.html#opal",
    "href": "help.html#opal",
    "title": "Get up to speed: Useful reads",
    "section": "Opal",
    "text": "Opal\nWe will be interacting with DataSHIELD through a data warehouse called Opal. This is the server that will handle the authentication of our credentials, storage of data and “resources” and will provide an R server where the non-disclosive analysis will be conducted. Information about it can also be foun online:\n\nOpal papers 1; 2\nOpal documentation"
  },
  {
    "objectID": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "href": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "title": "Get up to speed: Useful reads",
    "section": "“resources”: A very simple explanation without any technicalities",
    "text": "“resources”: A very simple explanation without any technicalities\nIt is quite important to have a solid understanding of what are the “resources” and how we work with them, since we will be using them to load our data on the R sessions. For that reason we included a very brief description of them without using technicalities.\nThe “resources” can be imagined as a data structure that contains the information about where to find a data set and the access credentials to it; we as DataSHIELD users are not able to look at this information (it is privately stored on the Opal server), but we can load it into our remote R session to make use of it. Following that, the next step comes naturally.\nOnce we have in an R session the information to access a dataset (an table for example) we have to actually retrieve it on the remote R session to analyze it. This step is called resolving the resource.\nThose two steps can be identified on the code we provide as the following:\nLoading the information of a “resource”:\n\nDSI::datashield.assign.resource(conns, \"resource\", \"resource.path.in.opal.server\")\n\nResolving the “resource”:\n\nDSI::datashield.assign.expr(conns, \"resource.resolved\", expr = as.symbol(\"as.resource.data.frame(resource)\"))\n\nThis toy code would first load the “resource” on a variable called resource and it would retrieve the information it contains and assign it to a variable called resource.resolved."
  },
  {
    "objectID": "workshop_part0.html",
    "href": "workshop_part0.html",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "",
    "text": "To begin, we will load the libraries that will allow us to connect to the Opal analysis servers.\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)"
  },
  {
    "objectID": "workshop_part0.html#log-in-to-the-study-servers",
    "href": "workshop_part0.html#log-in-to-the-study-servers",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "Log in to the study servers",
    "text": "Log in to the study servers\nWe begin by creating the connection object, to do that we will use the DSI package. It is at this stage that we can decide whether only connect to a single study server or to multiple. If we connect to multiple study servers we can make use of the pooled functionalities of some DataSHIELD functions.\n\nbuilder <- DSI::newDSLoginBuilder()\nbuilder$append(server = \"hm_hospitales\",\n               url = \"https://192.168.1.50:9002\",\n               user = \"user_analisis\", password = \"**********\")\nbuilder$append(server = \"sc_verona\",\n               url = \"https://192.168.1.50:8890\",\n               user = \"user_analisis\", password = \"**********\")\nbuilder$append(server = \"umf_cluj\",\n               url = \"https://192.168.1.200:8005\",\n               user = \"user_analisis\", password = \"**********\")\nbuilder$append(server = \"test_server\", # This server will fail!\n               url = \"https://192.168.1.1:8888\",\n               user = \"test\", password = \"**********\")\nlogindata <- builder$build()\n\n\n\n\nWe just created the logindata object, which contains all the login information, the next step is to use this information and actually connect to the servers.\n\nconnections <- DSI::datashield.login(logins = logindata)\n\n\nLogging into the collaborating servers\n\n\nError in curl::curl_fetch_memory(url, handle = handle): schannel: next InitializeSecurityContext failed: SEC_E_UNTRUSTED_ROOT (0x80090325) - La cadena de certificación fue emitida por una entidad en la que no se confía.\n\n\nThe error we are getting is due the study server having no SSL certificate, by default R refuses to connect to endpoints without SSL certificates. We do not need to know what an SSL certificate is or why is it important.\nWe can maually disable this limitation of R so that it accepts to connect.\n\nlibrary(httr)\nset_config(config(ssl_verifypeer = 0L, \n                  ssl_verifyhost = 0L))\n\nAnd now we successfully connect to the study servers.\n\nconnections <- DSI::datashield.login(logins = logindata, failSafe = TRUE)\n\n\nLogging into the collaborating servers\n\n\nWarning: test_server is excluded because login connection failed\n\n\nWe can see that if one of the servers fails to connect we get a warning message, but the successful connections get stored into the connections object."
  },
  {
    "objectID": "workshop_part1.html",
    "href": "workshop_part1.html",
    "title": "Part 1: Loading the data",
    "section": "",
    "text": "Now that we are connected to the study servers, we have to load the data on the remote R sessions."
  },
  {
    "objectID": "workshop_part1.html#loading-the-resources",
    "href": "workshop_part1.html#loading-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Loading the resources",
    "text": "Loading the resources\nWe will begin by loading the resources. To do so, we have to know the path to them in the Opal servers, if we do not know it, there are two different ways of finding out.\n\nOpal UI\nWe can use the web user interface of Opal. To do that, just go to your browser of choice and navigate to the server URL. Login with the same credentials you are using for DataSHIELD.\n\n\n\nOpal UI landing page\n\n\nOnce we login, we have to navigate to the Projects tab, there we will find the available projects on the Opal server, we have to write down the name of the project we are interested on using.\n\n\n\nOpal UI Projects page\n\n\nWe then click on the project of interest. On the resources tab we will see the available resources. We have to write down the resource of interest.\n\n\n\nOpal UI Available resources\n\n\n\n\nUsing R console\nWe can use the opalr package to interact with the Opal server and retrieve information about it. The information we are interested on is the available projects and the resources they contain.\nFirst we login to one Opal, if we are using multiple servers, we have to perform the same procedure for each one; we will illustrate the HM Hospitals server only.\n\no <- opalr::opal.login(username = \"user_analisis\", \n                  password = \"*********\", \n                  url = \"https://192.168.1.50:9002\")\n\n\n\n\nThen, we look for the available projects.\n\nopalr::opal.projects(o)\n\n  name title tags                  created               lastUpdate\n1 FiHM  FiHM   NA 2022-02-10T10:29:06.000Z 2022-02-10T10:29:06.000Z\n\n\nAnd finally, we look for the available resources inside this project.\n\nopalr::opal.resources(o, \"FiHM\")\n\n             name project\n1 harmonized_data    FiHM\n                                                                   url format\n1 opal+http://opal:8080/ws/files/projects/FiHM/HM_Harmonized_Dates.csv    csv\n               created              updated\n1 2022-04-25T09:51:27Z 2022-04-25T09:51:27Z\n\n\n\nopalr::opal.logout(o)\n\n\n\nProjects and resources names\nIf we perform the same operations for all three study centers we are using in this workshop, we can obtain the following information:\n\n\n\n\n\n\n\n\n\nStudy center\nURL\nProject Name\nResource Name\n\n\n\n\nHM Hospitales\nhttps://192.168.1.50:9002\nFiHM\nharmonized_data\n\n\nSacrocuore Verona\nhttps://192.168.1.50:8890\nS_uncover\nverona\n\n\nUMF Cluj\nhttps://192.168.1.200:8005\nvUMF_Cluj\nRomania\n\n\n\n\n\nLoading the resources in the remote R sessions\nNow we have to tell each study center to load the correspondent resource. What we have created on the previous part is a connection object that contains the connections to each study server. We can see how it looks.\n\nconnections\n\n$hm_hospitales\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"hm_hospitales\"\n\nSlot \"opal\":\nurl: https://192.168.1.50:9002 \nname: hm_hospitales \nversion: 4.0.3 \nusername: user_analisis \nprofile:  \n\n\n$sc_verona\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"sc_verona\"\n\nSlot \"opal\":\nurl: https://192.168.1.50:8890 \nname: sc_verona \nversion: 4.2.8 \nusername: user_analisis \nprofile:  \n\n\n$umf_cluj\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"umf_cluj\"\n\nSlot \"opal\":\nurl: https://192.168.1.200:8005 \nname: umf_cluj \nversion: 4.0.3 \nusername: user_analisis \nprofile:  \n\n\nIt is basically a typical R list, therefore we can interact with a single server (for example the Verona one) by using connections[2]. We will do that to send the appropiate information to each study server. The syntaxis to specifiy the path of the resource follows the structure project.resource_name.\n\nDSI::datashield.assign.resource(connections$hm_hospitales, \"resource\", \"FiHM.harmonized_data\")\nDSI::datashield.assign.resource(connections$sc_verona, \"resource\", \"S_uncover.verona\")\nDSI::datashield.assign.resource(connections$umf_cluj, \"resource\", \"UMF_Cluj.Romania\")\n\nNow, each study server has an object called resource with the correspondent information. So in the following function calls we don’t have to send different information to each server."
  },
  {
    "objectID": "workshop_part1.html#resolving-the-resources",
    "href": "workshop_part1.html#resolving-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Resolving the resources",
    "text": "Resolving the resources\nThe next step is to use the information of the resource object and load the dataset that it points to. In order do so, we just have to “resolve” this object. We do that with the following function.\n\nDSI::datashield.assign.expr(conns = connections, symbol = \"data\", \n                            expr = \"as.resource.data.frame(resource)\")\n\nWe have created an object called data. This object contains a dataframe with the data we will use to perform our analysis. We can check if that is true and the dimensions of this dataframe.\n\nds.class(\"data\")\n\n$hm_hospitales\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n$sc_verona\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n$umf_cluj\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nds.dim(\"data\")\n\n$`dimensions of data in hm_hospitales`\n[1] 6864  219\n\n$`dimensions of data in sc_verona`\n[1] 1515   45\n\n$`dimensions of data in umf_cluj`\n[1] 999 132\n\n$`dimensions of data in combined studies`\n[1] 9378  219"
  },
  {
    "objectID": "workshop_part2.html",
    "href": "workshop_part2.html",
    "title": "Part 2: Data validation",
    "section": "",
    "text": "Now that we have the data loaded, we can check if the data is compliant with the ranges described on the codebook. (IS IT PUBLIC/CAN WE LINK IT HERE?). On this step we can check that the categorical variables are properly encoded and that the numerical variables have the appropriate range. Regarding the numerical variables, we will not be checking the range, we will be looking the 5%/95% interquartile range, as there is no DataSHIELD function to output maximums and minimums."
  },
  {
    "objectID": "workshop_part2.html#columns-available",
    "href": "workshop_part2.html#columns-available",
    "title": "Part 2: Data validation",
    "section": "Columns available",
    "text": "Columns available\nFirst, we check the columns available on the loaded data.\n\ncolnames_servers <- ds.colnames(\"data\")\n\nSince not all study centers have collected the same variables, we can use the following script to see the common variables.\n\nReduce(intersect, colnames_servers)\n\n [1] \"DMRAGEYR\"       \"DATAD\"          \"CMXCVD\"         \"CMXCVD_numeric\"\n [5] \"CMXHT\"          \"CMXHT_numeric\"  \"CMXCKD\"         \"CMXCKD_numeric\"\n [9] \"CMXCLD\"         \"CMXCLD_numeric\" \"CMXCPD\"         \"CMXCPD_numeric\"\n[13] \"CMXRHE\"         \"CMXRHE_numeric\"\n\n\nAnother scenario may be the one where we know our variables of interest and want to check whether they are or not on the study centers. To do that we can use a function from the dsHelper package.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndsHelper::dh.classDiscrepancy(df = \"data\", vars = c(\"DSXOS\", \"CMXCLD\", \"RFXONC\"), conns = connections)\n\n# A tibble: 3 × 5\n  variable discrepancy hm_hospitales sc_verona umf_cluj \n  <chr>    <chr>       <chr>         <chr>     <chr>    \n1 DSXOS    yes         character     NULL      character\n2 CMXCLD   no          character     character character\n3 RFXONC   yes         NULL          character character\n\n\nThis function gives us information about the class of the selected variables on each study server. Also, when a certain variable is not available, it shows as NULL class. So with this function we can get information about data availability and harmonization."
  },
  {
    "objectID": "workshop_part2.html#dropping-a-study-server",
    "href": "workshop_part2.html#dropping-a-study-server",
    "title": "Part 2: Data validation",
    "section": "Dropping a study server",
    "text": "Dropping a study server\nOnce we are aware of the available variables, we might be interested on dropping one of the connections (or multiple). To do so, we have to close the desired connection. We will illustrate that we are dropping the Madrid study center.\n\ndatashield.logout(connections$hm_hospitales)\n\nNow the connection is closed, so we can remove the HM Hospitales server from our connections list.\n\nconnections$hm_hospitales <- NULL\n\nFrom this point onwards, the connections object will only access the Verona and Cluj study servers."
  },
  {
    "objectID": "workshop_part2.html#categorical-variables",
    "href": "workshop_part2.html#categorical-variables",
    "title": "Part 2: Data validation",
    "section": "Categorical variables",
    "text": "Categorical variables\nWe compare against the codebook and see that we have the CMXCVD variable, which should have the categories: 0 and 1 (plus missing).\n \nFirst, we can check the class of this variable.\n\nds.class(\"data$CMXCVD\")\n\n$sc_verona\n[1] \"character\"\n\n$umf_cluj\n[1] \"character\"\n\n\nWe might have expected to have a variable of class factor, later on this workshop we will see how to transform a variable to a factor, for the moment we do not have to worry. Having a character variable is enough for the data validation.\nFollowing that, we can extract an uni-dimensional contingency table of the variable. This way we will obtain the different categories of the variable and their counts. Aside, we will obtain the count of missings (NA) for this variable.\n\nCMSCVD_table <- ds.table(\"data$CMXCVD\")\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\nCMSCVD_table$output.list$TABLES.COMBINED_all.sources_counts\n\ndata$CMXCVD\n Yes   No   NA \n 248  143 2123 \n\nCMSCVD_table$output.list$TABLE_rvar.by.study_counts\n\n           study\ndata$CMXCVD sc_verona umf_cluj\n        Yes       139      109\n        No          0      143\n        NA       1376      747\n\n\nThis variable complies with the codebook."
  },
  {
    "objectID": "workshop_part2.html#numerical-variables",
    "href": "workshop_part2.html#numerical-variables",
    "title": "Part 2: Data validation",
    "section": "Numerical variables",
    "text": "Numerical variables\nFrom previously shown codebook screenshot, we can see that we also have the DMRAGEYR variable, which should be numeric. We can confirm that.\n\nds.class(\"data$DMRAGEYR\")\n\n$sc_verona\n[1] \"numeric\"\n\n$umf_cluj\n[1] \"numeric\"\n\n\nAs previously mentioned, we will check the 5%/95% interquartile range to see the range of the variable.\n\nds.quantileMean(\"data$DMRAGEYR\", type = \"split\")\n\n$sc_verona\n      5%      10%      25%      50%      75%      90%      95%     Mean \n16.70000 22.40000 38.00000 53.00000 68.00000 78.00000 82.30000 52.06601 \n\n$umf_cluj\n    5%    10%    25%    50%    75%    90%    95%   Mean \n40.450 46.900 54.250 65.000 72.000 80.100 84.000 63.832 \n\n\nThis is not the most useful way to check the range of a variable. There is an alternative provided by the dsHelper package, which is to exatract the (noise-perturbed) variable which would be used for the scatter plot. Depending on the security parameters of the Opal server, the noise added could be too severe to be useful, but with the default settings it serves as a great alternative.\n\nvariable <- dh.getAnonPlotData(df = \"data\", var_1 = \"DMRAGEYR\")\nhead(variable)\n\n# A tibble: 6 × 2\n  cohort    DMRAGEYR\n  <chr>        <dbl>\n1 sc_verona     70.0\n2 sc_verona     43.0\n3 sc_verona     54.0\n4 sc_verona     45.0\n5 sc_verona     67.0\n6 sc_verona     59.0\n\nvariable %>% \n  dplyr::group_by(cohort) %>% \n  dplyr::summarise(range = paste(round(range(DMRAGEYR)), collapse = \", \"))\n\n# A tibble: 3 × 2\n  cohort    range \n  <chr>     <chr> \n1 combined  10, 94\n2 sc_verona 10, 94\n3 umf_cluj  31, 92\n\n\nFinally, we can also check the amount of missings of the numerical variable.\n\nds.numNA(\"data$DMRAGEYR\")\n\n$sc_verona\n[1] 0\n\n$umf_cluj\n[1] 749"
  },
  {
    "objectID": "workshop_part4.html",
    "href": "workshop_part4.html",
    "title": "Part 3: Data wranggling",
    "section": "",
    "text": "We are now in the position to start working with the data. The first step is to learn how to manipulate it."
  },
  {
    "objectID": "workshop_part4.html#data-classes",
    "href": "workshop_part4.html#data-classes",
    "title": "Part 3: Data wranggling",
    "section": "Data classes",
    "text": "Data classes\nAll the tools to perform data transformations are the following:\n\nds.asCharacter: Useful to convert numerical values that we want as characters.\nds.asDataMatrixr: Useful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Maintains the original class for all columns.\nds.asMatrixUseful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Converts all columns into character class.\nds.asFactor: May cause disclosure issues if we try to convert a continuous variable. Very useful when the loaded categorical variables are represented as character instead of factor.\nds.asInteger: Useful when we require integer values, as the class numeric can’t guarantee it.\nds.asList: Rarely used.\nds.asLogical: To have bool variables. Similar to having factor variables.\nds.asNumeric: Useful to convert columns that have been interpreted as character class but we want them as numbers.\n\nWhen we use all this functions, we will create a new object on the study servers. If we are dealing with a data.frame and want to include this new column we created into it, we can use the following code.\n\nds.asFactor(input.var.name = \"data$CMXCVD\", newobj.name = \"CMXCVD_factor\")\n\n$all.unique.levels\n[1] \"Yes\" \"No\" \n\n$return.message\n[1] \"Data object <CMXCVD_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCVD_factor)\")\n\nWe will overwrite the input data object with an added column.\n\ntail(ds.colnames(\"data\")[[1]])\n\n[1] \"SMXSBA_numeric\"   \"SMXNAA_numeric\"   \"DMRGENDR_numeric\" \"DATAD_year\"      \n[5] \"DATAD_year_day\"   \"CMXCVD_factor\""
  },
  {
    "objectID": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "href": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "title": "Part 3: Data wranggling",
    "section": "Complex recoding. Number of comorbidities",
    "text": "Complex recoding. Number of comorbidities\nGiven the available variables on our dataset, we might be interested on recoding variables or creating new variables as combinations of the existing ones. This is exactly what we will do here. We will take the comorbities: CMXHT, CMXCVD, CMXCPD, CMXCKD, CMXCLD and RFXONC.\nWe will create a new variable called CMXCOM that quantifies how many comorbidities an individual has. We will have four categories:\n\n0 comorbidities\n1 comorbidities\n2 comorbidities\n3+ comorbidities\n\nThe process to achieve this is not as easy as straightforward as it would be using base R functions on our computer.\nTo begin, we will take a look at how the variables are encoded. Looking at the codebook they should all be dicotomous variables encoded as \"Yes\" / \"No\". We can verify that with the functions we’ve covered at the Part 2 of this workshop.\n\nds.table(\"data$CMXHT\")$output.list$TABLES.COMBINED_all.sources_counts\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\n\ndata$CMXHT\n Yes   No   NA \n 436  101 1977 \n\n\nNow, we will recode all the variabled so they are coded as 0 / 1.\n\nvariables <- c(\"CMXHT\", \"CMXCVD\", \n               \"CMXCPD\", \"CMXCKD\", \"CMXCLD\", \"RFXONC\")\n\nfor (x in variables){\n  ds.recodeValues(var.name = paste0(\"data$\", x), \n                  values2replace.vector = c(\"Yes\", \"No\"), \n                  new.values.vector = c(1, 0),\n                  newobj = paste0(x, \"_recoded\"))\n}\n\nWe have created a new object for each comorbiditie. The problem we now have is that the objects are of class character, because the ds.recodeValues function does not change the class.\n\nds.class(\"CMXHT_recoded\")\n\n$sc_verona\n[1] \"character\"\n\n$umf_cluj\n[1] \"character\"\n\n\nSince we want to count the numbers of comorbidities for each individual, it is important that we have a numeric variable. As we have prevously seen we can easily obtain this.\n\nfor (x in variables){\n  ds.asNumeric(x.name = paste0(x, \"_recoded\"), \n               newobj = paste0(x, \"_recoded_num\"))\n}\n\n\nds.class(\"CMXHT_recoded_num\")\n\n$sc_verona\n[1] \"numeric\"\n\n$umf_cluj\n[1] \"numeric\"\n\n\nNow we have a collection of variables encoded as we want and ready to be combined. First, we will join them on a data.frame.\n\nds.dataFrame(x = paste0(variables, \"_recoded_num\"), \n             newobj = \"joint_comorbidities\")\n\n$is.object.created\n[1] \"A data object <joint_comorbidities> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<joint_comorbidities> appears valid in all sources\"\n\n\nOn the server we have created a table that looks something like this:\n\n\n\n\n\n\n\n\n\n\nid\nRFXOB_recoded_num\nCMXDI_recoded_num\nCMXHT_recoded_num\n…\n\n\n\n\nIndividual 1\n0\n0\n0\n…\n\n\nIndividual 2\n1\n1\n1\n…\n\n\n…\n…\n…\n…\n…\n\n\n\nThe variable we want to create is the amount of comorbidities by individual, therefore we have to perform rowSums.\n\nds.rowColCalc(x = \"joint_comorbidities\", \n              operation = \"rowSums\", \n              newobj = \"new_variable\")\n\nWe are almost done, we will convert our new variable to be a factor.\n\nds.asFactor(input.var.name = \"new_variable\", \n            newobj.name = \"new_variable_factor\")\n\n$all.unique.levels\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\"\n\n$return.message\n[1] \"Data object <new_variable_factor> correctly created in all specified data sources\"\n\n\nWe can see that when we call the function ds.asFactor we receive the actual levels of the variable. We have said that the levels we actually want are 0, 1, 2, 3+.\nTo achieve this we just have to re-code the levels.\n\nds.recodeValues(var.name = \"new_variable_factor\", \n                values2replace.vector = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n                new.values.vector = c(\"0\", \"1\", \"2\",\"3+\", \"3+\", \"3+\",\"3+\"), \n                newobj = \"CMXCOM\")\n\n$is.object.created\n[1] \"A data object <CMXCOM> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<CMXCOM> appears valid in all sources\"\n\n\nFinally, we merge the new variable to our original data, and we are finished.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCOM)\")"
  },
  {
    "objectID": "workshop_part4.html#complete-cases",
    "href": "workshop_part4.html#complete-cases",
    "title": "Part 3: Data wranggling",
    "section": "Complete cases",
    "text": "Complete cases\nReal data tends to have missing values, however, some specific tools or functions expect complete data as input. For that reason there is a function specifically for that. Let’s suppose we want to have the complete cases for the variables DATAD and CMXHT. First we join them on a new data.frame.\n\nds.dataFrame(x = c(\"data$DATAD\", \"data$CMXHT\"), newobj = \"data_subset\")\n\n$is.object.created\n[1] \"A data object <data_subset> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset> appears valid in all sources\"\n\n\nNow we can get the complete cases.\n\nds.completeCases(x1 = \"data_subset\", newobj = \"data_subset_ccases\")\n\n$is.object.created\n[1] \"A data object <data_subset_ccases> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset_ccases> appears valid in all sources\"\n\n\nWe can check the dimensions before and after.\n\nds.dim(\"data_subset\")\n\n$`dimensions of data_subset in sc_verona`\n[1] 1515    2\n\n$`dimensions of data_subset in umf_cluj`\n[1] 999   2\n\n$`dimensions of data_subset in combined studies`\n[1] 2514    2\n\nds.dim(\"data_subset_ccases\")\n\n$`dimensions of data_subset_ccases in sc_verona`\n[1] 269   2\n\n$`dimensions of data_subset_ccases in umf_cluj`\n[1] 252   2\n\n$`dimensions of data_subset_ccases in combined studies`\n[1] 521   2"
  },
  {
    "objectID": "workshop_part4.html#dates",
    "href": "workshop_part4.html#dates",
    "title": "Part 3: Data wranggling",
    "section": "Dates",
    "text": "Dates\nFor this section we will use the HM Hospitales server, as it is the only one that has the prototype package installed.\n\n\n\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\nlibrary(dsDatesClient)\nbuilder <- DSI::newDSLoginBuilder()\nbuilder$append(server = \"hm_hospitales\",\n               url = \"https://192.168.1.50:9002\",\n               user = \"user_analisis\", password = \"Ekfl07UUgz\")\nlogindata <- builder$build()\nlibrary(httr);set_config(config(ssl_verifypeer = 0L))\nconnections <- DSI::datashield.login(logins = logindata, failSafe = TRUE)\n\n\nLogging into the collaborating servers\n\nDSI::datashield.assign.resource(connections, \"resource\", \"FiHM.harmonized_data\")\nDSI::datashield.assign.expr(conns = connections, symbol = \"data\", expr = \"as.resource.data.frame(resource)\")\n\nTo begin working with dates, first we have to convert the columns that have the dates to the appropiate class. On the unCoVer codebook, the dates are encoded as strings with the structure yyyy-mm-dd, this does not hold for the HM Hospitales server, which has the dates encoded as POSIXct, for that reason first we will convert them to characters. In future versions this will be solved and we will be able to work with POSIXct data. We convert the columns using the ds.asDate function.\n\nds.asCharacter(\"data$DATAD\", \"DATD_chr\")\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, DATD_chr)\")\nds.asCharacter(\"data$DATDS\", \"DATDS_chr\")\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, DATDS_chr)\")\n\nds.asDate(x.name = \"data$DATD_chr\", newobj = \"DATD_converted\", datasources = connections)\nds.asDate(x.name = \"data$DATDS_chr\", newobj = \"DATDS_converted\", datasources = connections)\n\nWe can now see that we have a column with the Date class.\n\nds.class(\"data$DATD_converted\")\n\n$hm_hospitales\n[1] \"Date\"\n\n\nNow we have two columns that contain dates in the correct class. With that, we can create a new column with the difference in days between the two dates.\n\nds.dateDiff(\"data$DATD_converted\", \"data$DATDS_converted\", \"difference\")\nds.quantileMean(\"data$difference\")\n\n Quantiles of the pooled data\n\n\n     5%     10%     25%     50%     75%     90%     95%    Mean \n 1.0000  2.0000  4.0000  7.0000 12.0000 20.0000 28.0000 10.0701 \n\n\nAlso, we can perform subsets of the dataframe using dates.\n\nds.subsetByDate(\"data\", \"DATD_converted\", year = 2022, newobj = \"data_subset\")\n\n$is.object.created\n[1] \"A data object <data_subset> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset> appears valid in all sources\"\n\nds.dim(\"data_subset\")\n\n$`dimensions of data_subset in hm_hospitales`\n[1] 549 224\n\n$`dimensions of data_subset in combined studies`\n[1] 549 224\n\n\nThe subset function can be used for more complex queries. For extra information make sure to check ?ds.subsetByDate.\n\nds.subsetByDate(\"data\", \"DATD_converted\", year = 2022, month = 4)\nds.subsetByDate(\"data\", \"DATD_converted\", range = c(\"2021-05-23\", \"2021-09-25\"))"
  },
  {
    "objectID": "workshop_part3.html",
    "href": "workshop_part3.html",
    "title": "Part 4: Descriptive analysis",
    "section": "",
    "text": "We have already checked that our data is compliant with the codebook. Now we can proceed with our analysis using the selected variables. To begin, we will perform a brief descriptive analysis, calculating some statistics, tables of contingence and doing some graphical visualizations."
  },
  {
    "objectID": "workshop_part3.html#descriptive-statistics",
    "href": "workshop_part3.html#descriptive-statistics",
    "title": "Part 4: Descriptive analysis",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nThere is a collection of DataSHIELD functions to get the descriptive statistics of a variable. Those are the functions ds.var, ds.mean, ds.table among others. However, there is a function from the dsHelper package that automatically performs all the function calls on the background, being a much more user-friendly alternative. The only downside is that it requires the categorical variables to be factors, we can easily create a new factor variable and add it as a new column to our data.\n\nds.asFactor(input.var.name = \"data$CMXHT\", newobj.name = \"CMXHT_factor\")\n\n$all.unique.levels\n[1] \"Yes\" \"No\" \n\n$return.message\n[1] \"Data object <CMXHT_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXHT_factor)\")\n\ndh.getStats(df = \"data\", vars = c(\"DMRAGEYR\", \"CMXHT_factor\"))\n\n$categorical\n# A tibble: 6 × 10\n  variable     cohort    category value cohort_n valid_n missing_n perc_valid\n  <chr>        <chr>     <fct>    <int>    <int>   <int>     <int>      <dbl>\n1 CMXHT_factor combined  Yes        436     2514     537      1977       81.2\n2 CMXHT_factor combined  No         101     2514     537      1977       18.8\n3 CMXHT_factor sc_verona Yes        285     1515     285      1230      100  \n4 CMXHT_factor sc_verona No           0     1515     285      1230        0  \n5 CMXHT_factor umf_cluj  Yes        151      999     252       747       59.9\n6 CMXHT_factor umf_cluj  No         101      999     252       747       40.1\n# … with 2 more variables: perc_missing <dbl>, perc_total <dbl>\n\n$continuous\n# A tibble: 3 × 15\n  variable cohort    mean std.dev perc_5 perc_10 perc_25 perc_50 perc_75 perc_90\n  <chr>    <chr>    <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 DMRAGEYR sc_vero…  52.1    20.0   16.7    22.4    38      53      68      78  \n2 DMRAGEYR umf_cluj  63.8    13.0   40.4    46.9    54.2    65      72      80.1\n3 DMRAGEYR combined  53.7    19.6   20.1    25.9    40.3    54.7    68.6    78.3\n# … with 5 more variables: perc_95 <dbl>, valid_n <dbl>, cohort_n <dbl>,\n#   missing_n <dbl>, missing_perc <dbl>"
  },
  {
    "objectID": "workshop_part3.html#graphical-visualizations",
    "href": "workshop_part3.html#graphical-visualizations",
    "title": "Part 4: Descriptive analysis",
    "section": "Graphical visualizations",
    "text": "Graphical visualizations\nThere is a rich collection of functions to perform graphical visualization of the variables. All the visualizations can be of the pooled data or separated by study.\n\nBoxplot\nThe boxplot is one of the newest plots available on DataSHIELD. It uses the ggplot2 library, providing lots of customization options.\n\nCombined plot\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\")\n\n\n\n\n\n\nStudy separated plot\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\", type = \"split\")\n\n\n\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\nGroupings\nThere are grouping options on the boxplot function which are very useful to have greater insights of the data. We can group using factor variables, so in this example we will use the previously created SMXAPA_factor variable.\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\", group = \"CMXHT_factor\")\n\n\n\n\nWe could even perform a second grouping using the argument group2 and stating another factor variable.\n\n\n\nHistogram\nTo have an idea of the distribution of a variable we can use histograms.\n\nhistogram <- ds.histogram(\"data$DMRAGEYR\")\n\n\n\n\n\n\nScatter plot\nDataSHIELD has a scatter plot functionality that outputs noise-affected data points, depending on the security configuration of the Opal, the noise levels can make this plot to be hugely distorted.\n\nds.scatterPlot(x = \"data$DMRAGEYR\", y = \"data$CSXCHRA\", datasources = connections)\n\n\n\n\n[1] \"Split plot created\"\n\n\n\n\nHeatmap plot\nIn a similar fashion than the scatter plot, we can visualize a two dimensional distribution of the points but in this case by density.\n\nds.heatmapPlot(x = \"data$DMRAGEYR\", y = \"data$CSXCHRA\")"
  },
  {
    "objectID": "workshop_part3.html#dimensional-contingency-table",
    "href": "workshop_part3.html#dimensional-contingency-table",
    "title": "Part 4: Descriptive analysis",
    "section": "2-Dimensional contingency table",
    "text": "2-Dimensional contingency table\nOn the previous part, we have already seen that DataSHIELD has a function to calculate uni-dimensional contingency tables. This same function, can also be used for bi-dimensional contingency tables. It is important to note that this function is a little bit tricky sometimes, as it is quite common that the 2D contingency table has disclosive outputs, therefore we just get an error message.\n\nds.table(\"data$DMRAGEYR\", \"data$SMXDIA_numeric\")\n\n\n All studies failed for reasons identified below \n\n\nStudy 1 :  Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3 \n\nStudy 2 :  Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3 \n\n\n$validity.message\n[1] \"All studies failed for reasons identified below\"\n\n$error.messages\n$error.messages$sc_verona\n[1] \"Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3\"\n\n$error.messages$umf_cluj\n[1] \"Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3\"\n\n\nThere are other variables that do produce valid non-disclosive results.\n\nctable <- ds.table(\"data$CMXCVD\", \"data$CMXCKD\")$output.list$TABLES.COMBINED_all.sources_counts\nctable\n\n           data$CMXCKD\ndata$CMXCVD Yes  No  NA\n        Yes  20  89   0\n        No   11 132   0\n        NA    0   0 747\n\n\n\nFisher test\nGiven the calculated contingency table, we are now in the position to perform a Fisher’s exact test.\n\nstats::fisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "href": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "title": "Part 4: Descriptive analysis",
    "section": "Statistics of a continuous variable grouped by a categorical variable",
    "text": "Statistics of a continuous variable grouped by a categorical variable\nWe may have interest on knowing the value of certain statistics of a variable when grouping with a category. As an example we will calculate the mean of the variable DMRAGEYR (Age) grouped by the variable CMXCOM (Number of comorbidities). Please note this is just an example, those two variables are not expected to be related.\nTo compute this statistic it is important that we have the categorical variable as a factor, otherwise the grouped statistic calculation will fail. Also, we have to create separate objects for the two variables, we can’t merge them into a data.frame and call them using the typical formulation data.frame$variable, for some reason it fails (to be reported as a bug to the core DataSHIELD team).\n\nds.assign(toAssign = \"data$DMRAGEYR\",\n            newobj =  \"DMRAGEYR\")\nds.assign(toAssign = \"data$CMXCOM\",\n            newobj =  \"CMXCOM\")\n\nds.tapply(X.name = \"DMRAGEYR\", INDEX.names = \"CMXCOM\", FUN.name = \"mean\")\n\n$sc_verona\n$sc_verona$Mean\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n 46.11960  65.47463  72.60241  78.80000 \n\n$sc_verona$N\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n     1087       335        83        10 \n\n\n$umf_cluj\n$umf_cluj$Mean\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n 53.53125  63.50704  68.98529  70.89362 \n\n$umf_cluj$N\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n       64        71        68        47 \n\n\nThis function can take other expressions on the argument FUN.name, more precisely:\n\n\"mean\"\n\"sd\"\n\"sum\"\n\"quantile\""
  },
  {
    "objectID": "workshop_part3.html#student-t-test",
    "href": "workshop_part3.html#student-t-test",
    "title": "Part 4: Descriptive analysis",
    "section": "Student t-test",
    "text": "Student t-test\nWe have two options to perform a t-test, and our choice will depend on whether we want to use pooled methods or we want to calculate the t-test on a single study server.\n\nPooled t-test\nTo perform a pooled t-test, we will use a collection of DataSHIELD functions to calculate the statistic.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWhere:\n\n\\(\\bar{x}\\) is the observed mean of the sample.\n\\(s\\) is the standard devuation of the sample.\n\\(n\\) is the sample size.\n\nWe will use the following code to perform this operations:\n\nmean1 <- ds.mean(\"data$DMRAGEYR\", type = \"c\")$Global.Mean[1]\nmean2 <- ds.mean(\"data$CSXOSTA\", type = \"c\")$Global.Mean[1]\nsd1 <- sqrt(ds.var(\"data$DMRAGEYR\", type = \"c\")$Global.Variance[1])\nsd2 <- sqrt(ds.var(\"data$CSXOSTA\", type = \"c\")$Global.Variance[1])\nn1 <- ds.length(\"data$DMRAGEYR\")[[3]] - sum(unlist(ds.numNA(\"data$DMRAGEYR\")))\nn2 <- ds.length(\"data$CSXOSTA\")[[3]] - sum(unlist(ds.numNA(\"data$CSXOSTA\")))\n\n(mean1 - mean2) / sqrt(sd1^2/n1 + sd2^2/n2)\n\n[1] -91.58199\n\n\nYou can expect to see a wrapper for this code on a future dsHelper release (dsHelper::dh.ttest('sample1', 'sample2').\n\n\nSingle study t-test\nIf we are not interested on using pooled functionalities, it is much simpler to perform a t-test.\n\nDSI::datashield.aggregate(connections, \"t.test(data$DMRAGEYR, data$CSXOSTA)\")\n\n$sc_verona\n\n    Welch Two Sample t-test\n\ndata:  data$DMRAGEYR and data$CSXOSTA\nt = -88.928, df = 1557.2, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -47.07162 -45.03991\nsample estimates:\nmean of x mean of y \n 52.06601  98.12177 \n\n\n$umf_cluj\n\n    Welch Two Sample t-test\n\ndata:  data$DMRAGEYR and data$CSXOSTA\nt = -31.201, df = 377.97, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -30.74986 -27.10393\nsample estimates:\nmean of x mean of y \n 63.83200  92.75889"
  },
  {
    "objectID": "workshop_part3.html#analysis-of-variance-anova",
    "href": "workshop_part3.html#analysis-of-variance-anova",
    "title": "Part 4: Descriptive analysis",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\nA student is developing a DataSHIELD function to perform ANOVAs. You can expect it to be available in the following months, probably as part of the dsML package."
  },
  {
    "objectID": "workshop_part5.html",
    "href": "workshop_part5.html",
    "title": "Part 5: Statistical models",
    "section": "",
    "text": "Now that we have covered the basics, we can begin to analyze the data in more depth.\nIn this section we will go over some of the statistical models we can fit by using DataSHIELD."
  },
  {
    "objectID": "workshop_part5.html#logistic-regression",
    "href": "workshop_part5.html#logistic-regression",
    "title": "Part 5: Statistical models",
    "section": "Logistic regression",
    "text": "Logistic regression\nWe will perform some logistic regressions analysis to see the association of different variables to mortality (DSXOS variable). To achieve this, we will have to apply some of the concepts we have already seen in order to prepare our data.\n\nPreparing the data\n\n\n\n\n\n\nWe can use the the variables age, gender, cholesterol, etc. We are going to use arbitrary variables that are common on the HM Hospitales and Cluj study servers for illustrating purposes.\n\n\n\nFor the logistic regression, the variables we will use are the following:\n\nDSXOS: Character variable. Outcome status encoded as Deceased, Recovered and Transferred. To perform a logistic regression this variable will have to be re-encoded as 1/0 (case/control); the case being Deceased and the controls being Recovered and Transferred.\nCMXCOM: Factor variable. Number of comorbidities, encoded as 0, 1, 2 and 3+.\nCMXCLD: Character variable. Chronic liver disease, encoded as Yes and No.\nCMXCPD: Character variable. Chronic pulmonary disease, encoded as Yes and No.\n\nWe only have to do some data wrangling for the DSXOS variable, first we do the re-coding.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = c(\"Deceased\", \"Recovered\", \"Transferred\"), \n                new.values.vector = c(1, 0, 0),\n                newobj = \"DSXOS_recoded\")\n\nError: There are some DataSHIELD errors, list them with datashield.errors()\n\ndatashield.errors()\n\n$umf_cluj\n[1] \"Command 'recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\n    \\\"1,0,0\\\", NULL)' failed on 'umf_cluj': Error while evaluating 'is.null(base::assign('DSXOS_recoded', value={dsBase::recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\\"1,0,0\\\", NULL)}))' -> Error : Error: values2replace.text argument too long (see nfilter.stringShort)\\n\"\n\n$hm_hospitales\n[1] \"Command 'recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\n    \\\"1,0,0\\\", NULL)' failed on 'hm_hospitales': Error while evaluating 'is.null(base::assign('DSXOS_recoded', value={dsBase::recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\\"1,0,0\\\", NULL)}))' -> Error : Error: values2replace.text argument too long (see nfilter.stringShort)\\n\"\n\n\nWe can see that the DataSHIELD filter is complaining that the length of the values2replace.vector is too large. To overcome this issue, we can perform separate function calls for each of the levels.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = \"Deceased\", \n                new.values.vector = 1,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Recovered\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Transferred\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\n\nBy doing that we successfully created the object DSXOS_recoded, now we just have to convert it to numerical.\n\nds.asNumeric(x.name = \"DSXOS_recoded\", newobj = \"DSXOS_recoded_num\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded_num> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded_num> appears valid in all sources\"\n\n\n\nds.summary(\"DSXOS_recoded_num\")\n\n$umf_cluj\n$umf_cluj$class\n[1] \"numeric\"\n\n$umf_cluj$length\n[1] 999\n\n$umf_cluj$`quantiles & mean`\n       5%       10%       25%       50%       75%       90%       95%      Mean \n0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 1.0000000 1.0000000 0.1304348 \n\n\n$hm_hospitales\n$hm_hospitales$class\n[1] \"numeric\"\n\n$hm_hospitales$length\n[1] 6864\n\n$hm_hospitales$`quantiles & mean`\n       5%       10%       25%       50%       75%       90%       95%      Mean \n0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 1.0000000 1.0000000 0.1107903 \n\nds.numNA(\"DSXOS_recoded_num\")\n\n$umf_cluj\n[1] 746\n\n$hm_hospitales\n[1] 284\n\n\nFinally, we add it to our data.frame with the other covariates, and we are ready to perform the logistic regression.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, DSXOS_recoded_num)\")\n\n\n\nFitting the models\n\nPooled analysis\nNow we can fit the models. First, we will calculate whether chronic liver disease anc chronic pulmonary disease are associated with higher risk of mortality.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCLD + CMXCPD\", \n       data = \"data\", \n       family = \"binomial\")$coefficients\n\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -2.1789095 0.04339820 -50.207371 0.000000e+00   -2.2639684\nCMXCLDYes    0.4954367 0.49518245   1.000513 3.170621e-01   -0.4751031\nCMXCPDYes    0.5749502 0.09502311   6.050636 1.442753e-09    0.3887084\n            high0.95CI.LP      P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)    -2.0938506 0.1016605     0.09415137        0.109696\nCMXCLDYes       1.4659764 1.6412148     0.62182095        4.331771\nCMXCPDYes       0.7611921 1.7770421     1.47507430        2.140827\n\n\nAnd we also calculate the association of the outcome status to the number of comorbidities.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCOM\", \n       data = \"data\", \n       family = \"binomial\")$coefficients\n\n              Estimate Std. Error    z-value       p-value low0.95CI.LP\n(Intercept) -2.8013604 0.07634247 -36.694653 8.887664e-295   -2.9509889\nCMXCOM1      0.9451591 0.09699454   9.744457  1.948171e-22    0.7550533\nCMXCOM2      1.3683043 0.11142871  12.279639  1.165177e-34    1.1499080\nCMXCOM3+     1.6463952 0.15775657  10.436302  1.692779e-25    1.3371980\n            high0.95CI.LP       P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)     -2.651732 0.05725071      0.0496898      0.06588235\nCMXCOM1          1.135265 2.57322286      2.1277250      3.11199795\nCMXCOM2          1.586701 3.92868299      3.1579024      4.88759571\nCMXCOM3+         1.955592 5.18824326      3.8083574      7.06810461\n\n\n\n\nMeta-analysis\nTo fit the same models to be meta-analyzed, we just have to use a different function with the same structure.\n\nmod1 <- ds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCLD + CMXCPD\", \n                   dataName = \"data\", \n                   family = \"binomial\")\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\nmod1$output.summary$study1$coefficients\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -1.9692942  0.2151323 -9.1538757 5.492676e-20\nCMXCLDYes    0.4102159  0.5341748  0.7679432 4.425209e-01\nCMXCPDYes    0.1766349  0.5276192  0.3347771 7.377932e-01\n\nmod1$output.summary$study2$coefficients\n\n              Estimate   Std. Error      z value     Pr(>|z|)\n(Intercept) -2.1873606   0.04432877 -49.34404098 0.000000e+00\nCMXCLDYes   -9.3786929 196.96768400  -0.04761539 9.620228e-01\nCMXCPDYes    0.5902685   0.09665529   6.10694500 1.015562e-09\n\n\n\nmod2 <- ds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCOM\", \n                   dataName = \"data\", \n                   family = \"binomial\")$output.summary$study1$coefficients\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\n\nAt this moment, the consortia is using dsBaseClient 6.1.1. In the new version 6.2, there is a function to visualize the meta-analyzed coefficients using forestplots."
  },
  {
    "objectID": "workshop_part5.html#poisson-regression",
    "href": "workshop_part5.html#poisson-regression",
    "title": "Part 5: Statistical models",
    "section": "Poisson Regression",
    "text": "Poisson Regression\na veces el outcome no es binario sino que es una variable de conteo\nThe same functions we just used to fit logistical regressions, can also fit piecewise exponential regressions. This is achieved by selecting the output family to be of type poisson. For the variable DATLGT (Length of stay in hospital), we can check that it does follow a Poisson distribution.\n\nhistogram <- ds.histogram(\"data$DATLGT\")\n\nWarning: umf_cluj: 1 invalid cells\n\n\nWarning: hm_hospitales: 0 invalid cells\n\n\n\n\n\nAnd we can fit the model.\n\nds.glm(formula = \"DATLGT ~ CMXCOM\", \n       data = \"data\", \n       family = \"poisson\")$coefficients\n\n             Estimate  Std. Error   z-value      p-value low0.95CI.LP\n(Intercept) 2.2098088 0.005767066 383.17728 0.000000e+00    2.1985056\nCMXCOM1     0.1682207 0.008383213  20.06637 1.452320e-89    0.1517899\nCMXCOM2     0.2119990 0.010975000  19.31654 3.899437e-83    0.1904884\nCMXCOM3+    0.2149337 0.018283395  11.75568 6.602924e-32    0.1790989\n            high0.95CI.LP EXPONENTIATED RR low0.95CI.EXP high0.95CI.EXP\n(Intercept)     2.2211121         9.113974      9.011537       9.217576\nCMXCOM1         0.1846515         1.183198      1.163916       1.202799\nCMXCOM2         0.2335096         1.236147      1.209840       1.263025\nCMXCOM3+        0.2507685         1.239780      1.196139       1.285013"
  },
  {
    "objectID": "workshop_part5.html#survival-analysis",
    "href": "workshop_part5.html#survival-analysis",
    "title": "Part 5: Statistical models",
    "section": "Survival analysis",
    "text": "Survival analysis\n\n\n\n\n\n\nThe variable selection and survival analysis sections will only use the HM Hospitals study server. This is due because at the time of writting this material it is the only study server with the dsSurvival and dsMTLBase packages installed.\n\n\n\n\ndatashield.logout(connections$umf_cluj)\nconnections$umf_cluj <- NULL\n\nIt is expected that the survival rate of patients is lower as the stay at hospital is increased. Here we will check this argument.\nFirst, we will prepare our data. For the dsSurvival package, we have to separate into different objects the event variable (DSXOS_recoded_num) and the survival time variable (DATLGT). Both variables have to be numeric.\n\nds.make(toAssign = \"data$DSXOS_recoded_num\", newobj = \"EVENT\")\n\n$is.object.created\n[1] \"A data object <EVENT> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<EVENT> appears valid in all sources\"\n\nds.make(toAssign = \"data$DATLGT\", newobj = \"SURVTIME\")\n\n$is.object.created\n[1] \"A data object <SURVTIME> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<SURVTIME> appears valid in all sources\"\n\n\nAnd now we can perform the survival analysis using Cox proportional hazards model.\n\ndsSurvivalClient::ds.coxph.SLMA(formula = 'survival::Surv(time=SURVTIME,event=EVENT)~data$DMRGENDR',\n                                               dataName = 'data',\n                                               datasources = connections)\n\nssssstimelllSURVTIMErrreventlllEVENTzzz~data$DMRGENDR\n\n\n$hm_hospitales\n$call\nsurvival::coxph(formula = formula, data = dataTable, weights = weights, \n    ties = ties, singular.ok = singular.ok, model = model, x = x, \n    y = y)\n\n$fail\nNULL\n\n$na.action\n  18   57  139  268  313  336  356  364  370  371  382  394  432  475  487  513 \n  18   57  139  268  313  336  356  364  370  371  382  394  432  475  487  513 \n 515  528  555  565  577  578  601  629  634  638  656  682  688  691  694  705 \n 515  528  555  565  577  578  601  629  634  638  656  682  688  691  694  705 \n 709  721  733  736  751  847  854  864  926  973 1157 1332 1365 1417 1419 1420 \n 709  721  733  736  751  847  854  864  926  973 1157 1332 1365 1417 1419 1420 \n1443 1516 1526 1551 1604 1616 1626 1649 1650 1718 1756 1763 1802 1810 1829 1835 \n1443 1516 1526 1551 1604 1616 1626 1649 1650 1718 1756 1763 1802 1810 1829 1835 \n1858 1870 2119 2120 2174 2221 2294 2387 2401 2402 2412 2416 2420 2421 2424 2440 \n1858 1870 2119 2120 2174 2221 2294 2387 2401 2402 2412 2416 2420 2421 2424 2440 \n2444 2458 2460 2470 2489 2495 2521 2528 2532 2538 2547 2548 2551 2564 2571 2572 \n2444 2458 2460 2470 2489 2495 2521 2528 2532 2538 2547 2548 2551 2564 2571 2572 \n2584 2585 2597 2600 2602 2603 2628 2631 2652 2671 2675 2677 2688 2726 2761 2783 \n2584 2585 2597 2600 2602 2603 2628 2631 2652 2671 2675 2677 2688 2726 2761 2783 \n2811 2829 2882 2883 3029 3093 3145 3326 3436 3444 3445 3449 3458 3460 3475 3485 \n2811 2829 2882 2883 3029 3093 3145 3326 3436 3444 3445 3449 3458 3460 3475 3485 \n3502 3509 3520 3528 3531 3535 3548 3549 3557 3576 3579 3584 3630 3632 3644 3645 \n3502 3509 3520 3528 3531 3535 3548 3549 3557 3576 3579 3584 3630 3632 3644 3645 \n3647 3704 3723 3728 3745 3749 3750 3751 3775 3776 3794 3805 3813 3814 3854 3856 \n3647 3704 3723 3728 3745 3749 3750 3751 3775 3776 3794 3805 3813 3814 3854 3856 \n3866 3871 3930 3956 4196 4224 4246 4256 4328 4336 4342 4347 4381 4387 4411 4430 \n3866 3871 3930 3956 4196 4224 4246 4256 4328 4336 4342 4347 4381 4387 4411 4430 \n4461 4487 4502 4555 4557 4558 4564 4567 4587 4594 4626 4656 4670 4676 4697 4758 \n4461 4487 4502 4555 4557 4558 4564 4567 4587 4594 4626 4656 4670 4676 4697 4758 \n4792 4861 4869 4870 4883 4933 4958 4975 5118 5137 5171 5173 5207 5208 5212 5311 \n4792 4861 4869 4870 4883 4933 4958 4975 5118 5137 5171 5173 5207 5208 5212 5311 \n5323 5344 5345 5354 5394 5400 5404 5410 5411 5455 5462 5468 5469 5479 5488 5489 \n5323 5344 5345 5354 5394 5400 5404 5410 5411 5455 5462 5468 5469 5479 5488 5489 \n5553 5560 5587 5627 5629 5722 5732 5740 5741 5742 5743 5748 5751 5756 5762 5773 \n5553 5560 5587 5627 5629 5722 5732 5740 5741 5742 5743 5748 5751 5756 5762 5773 \n5774 5785 5805 5833 5860 5907 5978 6003 6007 6076 6096 6117 6179 6276 6303 6304 \n5774 5785 5805 5833 5860 5907 5978 6003 6007 6076 6096 6117 6179 6276 6303 6304 \n6307 6326 6333 6357 6358 6372 6384 6405 6428 6440 6442 6443 6491 6505 6528 6530 \n6307 6326 6333 6357 6358 6372 6384 6405 6428 6440 6442 6443 6491 6505 6528 6530 \n6553 6559 6579 6585 6641 6696 6751 6790 6805 6808 6848 6849 \n6553 6559 6579 6585 6641 6696 6751 6790 6805 6808 6848 6849 \nattr(,\"class\")\n[1] \"omit\"\n\n$n\n[1] 6580\n\n$loglik\n[1] -5491.543 -5491.536\n\n$nevent\n[1] 729\n\n$coefficients\n                         coef exp(coef)   se(coef)         z Pr(>|z|)\ndata$DMRGENDRMale 0.008906727  1.008947 0.07672496 0.1160864 0.907584\n\n$conf.int\n                  exp(coef) exp(-coef) lower .95 upper .95\ndata$DMRGENDRMale  1.008947  0.9911328   0.86808  1.172672\n\n$logtest\n      test         df     pvalue \n0.01348575 1.00000000 0.90755097 \n\n$sctest\n      test         df     pvalue \n0.01347615 1.00000000 0.90758374 \n\n$rsq\n         rsq       maxrsq \n2.049504e-06 8.115951e-01 \n\n$waldtest\n    test       df   pvalue \n0.010000 1.000000 0.907584 \n\n$used.robust\n[1] FALSE\n\n$concordance\n         C      se(C) \n0.49383283 0.01135604 \n\nattr(,\"class\")\n[1] \"summary.coxph\"\n\n\nAlso, we can visualize the survival curves.\n\ndsSurvivalClient::ds.survfit(formula = \"survival::Surv(time=SURVTIME,event=EVENT)~data$DMRGENDR\", objectname = \"scurves\")\n\nError: There are some DataSHIELD errors, list them with datashield.errors()\n\nlibrary(survival)\ndsSurvivalClient::ds.plotsurvfit(formula = \"scurves\")\n\nError: There are some DataSHIELD errors, list them with datashield.errors()\n\n\nIn the near future, the survival curves will have the options to be visualized by a categorical variables (gender, for example). Also the error will be fixed, we are working with the developer of the package (Soumya Banerjee) on new features and bug fixes."
  },
  {
    "objectID": "workshop_part5.html#variable-selection-lasso",
    "href": "workshop_part5.html#variable-selection-lasso",
    "title": "Part 5: Statistical models",
    "section": "Variable selection: Lasso",
    "text": "Variable selection: Lasso\nWe have seen with the survival analysis how the variable DATLGT (Length of stay in hospital) affects the mortality. Now we will use a Lasso regression to analyze which other variables affect the mortality.\nBefore, we have to work a little bit with our data. The inputs to the Lasso regression have to be numerical variables without missings.\nWe will begin by removing all the missings in our dataset.\n\nds.completeCases(x1 = \"data\", newobj = \"data_complete\")\n\n$is.object.created\n[1] \"A data object <data_complete> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_complete> appears valid in all sources\"\n\n\nNow, we will create a subset with all the numeric variables.\n\ntypes <- lapply(ds.colnames(\"data_complete\")[[1]], function(x){\n  ds.class(paste0(\"data_complete$\", x))[[1]][1]\n})\ntypes <- unlist(types)\n`%notin%` <- Negate(`%in%`)\nindexes_to_remove <- which(types %notin% \"numeric\")\ntimes <- ds.dim(\"data_complete\")[[1]][1]\nds.rep(x1 = 1,\n            times = times,\n            source.times = \"c\",\n            source.each = \"c\",\n            newobj = \"ONES\")\n\n$is.object.created\n[1] \"A data object <ONES> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<ONES> appears valid in all sources\"\n\nds.dataFrameSubset(df.name = 'data_complete',  V1.name = \"ONES\",  V2.name = \"ONES\",  Boolean.operator = \"==\",keep.cols = NULL,  rm.cols = indexes_to_remove,  keep.NAs = NULL,  newobj = 'data_complete_numeric',  datasources = connections, notify.of.progress = FALSE)\n\n$is.object.created\n[1] \"A data object <data_complete_numeric> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_complete_numeric> appears valid in all sources\"\n\n\nNow we have all the variables (objective variables and covariates) in a single data.frame, we have to split it in two different objects. To drop columns, we have to know the column positions; we will drop the objective variable as well as the IDs variable.\n\nds.colnames(\"data_complete_numeric\")\n\n$hm_hospitales\n  [1] \"X\"                 \"IDINGRESO\"         \"DMRAGEYR\"         \n  [4] \"DMRGENDR_numeric\"  \"DATLGT\"            \"DATLGTI\"          \n  [7] \"DSXHO_numeric\"     \"DSXIC_numeric\"     \"DSXOS_numeric\"    \n [10] \"CMXPRG_numeric\"    \"CMXCVD_numeric\"    \"CMXCMP_numeric\"   \n [13] \"CMXHT_numeric\"     \"CMXDI_numeric\"     \"CMXCKD_numeric\"   \n [16] \"CMXCLD_numeric\"    \"CMXCPD_numeric\"    \"CMXASM_numeric\"   \n [19] \"CMXCND_numeric\"    \"CMXRHE_numeric\"    \"CMXCBD_numeric\"   \n [22] \"CMXDE_numeric\"     \"CMXPU_numeric\"     \"CMXST_numeric\"    \n [25] \"CMXLY_numeric\"     \"CMXAP_numeric\"     \"RFXSM_numeric\"    \n [28] \"RFXFSM_numeric\"    \"RFXOB_numeric\"     \"RFXTB_numeric\"    \n [31] \"RFXIMD_numeric\"    \"RFXHIV_numeric\"    \"RFXAIDS_numeric\"  \n [34] \"RFXUI_numeric\"     \"RFXHC_numeric\"     \"RFXMN_numeric\"    \n [37] \"CMXCHF_numeric\"    \"CMXVDP_numeric\"    \"CMXCA_numeric\"    \n [40] \"CMXVD_numeric\"     \"CMXABL_numeric\"    \"CMXAD_numeric\"    \n [43] \"CMXNDP_numeric\"    \"CMXNDD_numeric\"    \"CMXCOPD_numeric\"  \n [46] \"CMXPCD_numeric\"    \"CMXRI_numeric\"     \"CMXMLD_numeric\"   \n [49] \"CMXNPM_numeric\"    \"CMXCLH_numeric\"    \"CMXMET_numeric\"   \n [52] \"CMXTU_numeric\"     \"CMXMY_numeric\"     \"CMXNP_numeric\"    \n [55] \"RFXMD_numeric\"     \"CMXANX_numeric\"    \"CMXOAR_numeric\"   \n [58] \"CMXSOT_numeric\"    \"CMXUTI_numeric\"    \"CMXINC_numeric\"   \n [61] \"CMXHGL_numeric\"    \"RFXPM_numeric\"     \"RFXAP_numeric\"    \n [64] \"CMXIHD_numeric\"    \"COXRD_numeric\"     \"COXAR_numeric\"    \n [67] \"COXPM_numeric\"     \"COXMOD_numeric\"    \"COXPT_numeric\"    \n [70] \"COXEC_numeric\"     \"COXSH_numeric\"     \"COXIO_numeric\"    \n [73] \"COXPE_numeric\"     \"COXST_numeric\"     \"COXDIC_numeric\"   \n [76] \"COXRIO_numeric\"    \"COXKF_numeric\"     \"COXHF_numeric\"    \n [79] \"COXRF_numeric\"     \"COXLF_numeric\"     \"COXADE_numeric\"   \n [82] \"COXSTN_numeric\"    \"COXNOC_numeric\"    \"TRXHEP_numeric\"   \n [85] \"TRXAV_numeric\"     \"TRXLR_numeric\"     \"TRXRM_numeric\"    \n [88] \"TRXIB_numeric\"     \"TRXCH_numeric\"     \"TRXAB_numeric\"    \n [91] \"TRXCS_numeric\"     \"TRXAF_numeric\"     \"TRXNO_numeric\"    \n [94] \"TRXOX_numeric\"     \"TRXVA_numeric\"     \"TRXNMB_numeric\"   \n [97] \"TRXAC_numeric\"     \"TRXIS_numeric\"     \"TRXIM_numeric\"    \n[100] \"TRXVC_numeric\"     \"TRXVD_numeric\"     \"TRXZN_numeric\"    \n[103] \"TRXOT_numeric\"     \"TRXECM_numeric\"    \"TRXIV_numeric\"    \n[106] \"TRXRR_numeric\"     \"TRXTR_numeric\"     \"TRXPE_numeric\"    \n[109] \"TRXIT_numeric\"     \"DATAD_year\"        \"DATAD_year_day\"   \n[112] \"DSXOS_recoded_num\"\n\n\nThe columns we want to remove are the 1, 2, 9 and 112.\n\nds.assign(toAssign='data_complete_numeric$DSXOS_recoded_num', newobj='Y', datasources = connections)\n\nds.dataFrameSubset(df.name = 'data_complete_numeric',  V1.name = \"ONES\",  V2.name = \"ONES\",  Boolean.operator = \"==\",keep.cols = NULL,  rm.cols = c(1, 2, 9, 112),  keep.NAs = NULL,  newobj = 'X',  datasources = connections, notify.of.progress = FALSE)\n\n$is.object.created\n[1] \"A data object <X> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<X> appears valid in all sources\"\n\n\nFinally, the Lasso regression expects to recieve matrix objects instead of data.frame. We convert them.\n\nds.asMatrix(x.name = 'Y', newobj = 'Y')\n\nWarning in is.null(object.info[[j]]$test.obj.class) || object.info[[j]]\n$test.obj.class == : 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\n$is.object.created\n[1] \"A data object <Y> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<Y> appears valid in all sources\"\n\nds.asMatrix(x.name = 'X', newobj = 'X')\n\nWarning in is.null(object.info[[j]]$test.obj.class) || object.info[[j]]\n$test.obj.class == : 'length(x) = 2 > 1' in coercion to 'logical(1)'\n\n\n$is.object.created\n[1] \"A data object <X> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<X> appears valid in all sources\"\n\n\nWe are now in the position to successfully perform the variable selection. Before, we have to setup some configuration parameters. There is an argument called C, which is the hyperparameter for the Ridge regression L2 penalty. Thus, by tuning both the L1 penalty and the C argument (L2 term), one would be able to chose between a Lasso, Ridge or Elastic-Net regression. Likewise, there is an argument called opts, which allow controlling the optimization algorithm employed to minimize the sum of squared errors (SSE) (objective function). Additional details regarding the loss function implemented in these models can be found in the supplementary material of Cao et. al.. Within the opts argument we find:\n\ninit: It determines the starting point.\nmaxIter: It is the maximized iteration number.\nter: It refers to the termination rule used to determine the convergence of the algorithm. There are three termination rules available for ds.lasso. The first rule checks whether the current objective value was close enough to 0. The second rule investigates the last two objective values and checks whether the decline was close enough to 0. The third rule allowed the optimization to be performed for a certain maximum number of iterations.\ntol: It refers to the precision of the convergence and determines the termination of the program.\n\nWe will use the default parameters.\n\nopts=list();opts$init=0; opts$maxIter=10; opts$tol=0.01; opts$ter=2;\n\nFinally we can fit the Lasso regression.\n\nset.seed(123)\nm1=dsMTLClient::ds.LS_Lasso(X='X', Y='Y', lam=0.5, C=0, opts, datasources=connections, nDigits=15)\n\nWe can show the number of selected variables.\n\nsum(m1$w!=0)\n\n[1] 5\n\n\nVisually, the variable selection looks like this.\n\nplot(m1$w, xlab=\"index of coefficients\", ylab=\"values\")\n\n\n\n\nAnd, to conclude, to know which are the variables that the Lasso regression has selected.\n\nds.colnames(\"X\")[[1]][which(m1$w!=0)]\n\n[1] \"DMRAGEYR\"       \"DATLGT\"         \"DATLGTI\"        \"DATAD_year\"    \n[5] \"DATAD_year_day\"\n\n\n\n\n\n\n\n\nPlease, note that input variables are not standardized previous to the modeling.\n\n\n\nIt is important to remember that the dsMTLClient package has more capabilities than the one illustrated:\n\nds.LS_Lasso(): Solver of regression with Lasso.\nds.LR_Lasso(): Solver of logistic regression with Lasso.\n\nds.Lasso_Train(): Train a regularization tree with Lasso for a sequence of penalty values (than can be either provided by the user, or directly estimated from the data).\nds.Lasso_CVInSite(): In-site cross-validation procedure for selecting the optimal penalty."
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "ExerciseSolution\n\n\n\nBuild a connection object to a single study server: UMF Cluj (https://192.168.1.200:8005)\nLogin using the connection object\n\n\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\nbuilder <- DSI::newDSLoginBuilder()\nbuilder$append(server = \"verona\",\n              url = \"https://192.168.1.50:8890\",\n              user = \"user_analisis\", password = \"********\")\nlogindata <- builder$build()\nlibrary(httr);set_config(config(ssl_verifypeer = 0L))\nconnections <- DSI::datashield.login(logins = logindata)"
  },
  {
    "objectID": "exercises.html#exercise-2-assign-data",
    "href": "exercises.html#exercise-2-assign-data",
    "title": "Exercises",
    "section": "Exercise 2: Assign data",
    "text": "Exercise 2: Assign data\nUsing the connection object from the Exercise 1:\n\nExerciseSolution\n\n\n\nFind the available projects.\nFind the available resources inside the projects.\nLoad and resolve the resouce.\nWhat type of object is the loaded resource?\nWhat are the names of the available variables in that object?\nHow many individuals does it contain? And variables?\n\n\n\no <- opalr::opal.login(username = \"user_analisis\",\n                  password = \"*********\",\n                  url = \"https://192.168.1.50:8890\")\nopalr::opal.projects(o)\nopalr::opal.resources(o, \"UMF_Cluj\")\nopalr::opal.resources(o, \"S_uncover\")\nDSI::datashield.assign.resource(connections, \"resource\", \"S_uncover.verona\")\nDSI::datashield.assign.expr(conns = connections, symbol = \"data\",\n                            expr = \"as.resource.data.frame(resource)\")\nds.class(\"data\")\nds.colnames(\"data\")\nds.dim(\"data\")"
  },
  {
    "objectID": "exercises.html#exercise-3-data-validation",
    "href": "exercises.html#exercise-3-data-validation",
    "title": "Exercises",
    "section": "Exercise 3: Data validation",
    "text": "Exercise 3: Data validation\nUsing the connection object from the Exercise 1 and 2:\n\nExerciseSolution\n\n\n\nWhat is the class of the variables CSXRRA and TRXTR?\nWhat is the range of the variable CSXRRA?\nWhat are the categories of the variable TRXTR?\n\n\n\nds.class(\"data$CSXRRA\")\nds.class(\"data$TRXTR\")\n\nds.table(\"data$TRXTR\")\nds.summary(\"data$CSXRRA\")"
  },
  {
    "objectID": "exercises.html#exercise-4-data-wranggling",
    "href": "exercises.html#exercise-4-data-wranggling",
    "title": "Exercises",
    "section": "Exercise 4: Data wranggling",
    "text": "Exercise 4: Data wranggling\nUsing the connection object from the Exercise 1 and 2:\n\nExerciseSolution\n\n\n\nSelect 4 different categorical variables (Yes/No)\nCreate a new variable COUNTS that has the count for Yes.\nRecode the variable with the levels 1, 2+.\nAdd the new variable to the original dataset.\n\n\n\nds.class(\"data$TRXIS\")\nds.class(\"data$DSXIC\")\nds.class(\"data$CMXDI\")\nds.class(\"data$CMXHT\")\nds.table(\"data$TRXIS\")\nds.table(\"data$DSXIC\")\nds.table(\"data$CMXDI\")\nds.table(\"data$CMXHT\")\n\nvariables <- c(\"TRXIS\", \"DSXIC\", \n               \"CMXDI\", \"CMXHT\")\n\nfor (x in variables){\n  ds.recodeValues(var.name = paste0(\"data$\", x), \n                  values2replace.vector = c(\"Yes\", \"No\"), \n                  new.values.vector = c(1, 0),\n                  newobj = paste0(x, \"_recoded\"))\n}\n\nfor (x in variables){\n  ds.asNumeric(x.name = paste0(x, \"_recoded\"), \n               newobj = paste0(x, \"_recoded_num\"))\n}\n\nds.dataFrame(x = paste0(variables, \"_recoded_num\"), \n             newobj = \"joint_comorbidities\")\n             \nds.rowColCalc(x = \"joint_comorbidities\", \n              operation = \"rowSums\", \n              newobj = \"new_variable\")\n              \nds.asFactor(input.var.name = \"new_variable\", \n            newobj.name = \"new_variable_factor\")\n\nds.recodeValues(var.name = \"new_variable_factor\", \n                values2replace.vector = c(\"0\", \"1\", \"2\", \"3\", \"4\"),\n                new.values.vector = c(\"0\", \"1\", \"2+\", \"2+\", \"2+\"), \n                newobj = \"COUNTS\")\n                \nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, COUNTS)\")"
  },
  {
    "objectID": "exercises.html#exercise-5-descriptive-analysis",
    "href": "exercises.html#exercise-5-descriptive-analysis",
    "title": "Exercises",
    "section": "Exercise 5: Descriptive analysis",
    "text": "Exercise 5: Descriptive analysis\nUsing the connection object from the Exercise 1 and 2:\n\nExerciseSolution\n\n\n\nPerform a boxplot of the variable CSXRRA.\nPerform a boxplot of the variable CSXRRA grouped by CMXCPD.\nPerform a boxplot of the variable CSXRRA grouped by CMXCPD and CMXCLD.\nCalculate contingency table of the variables CMXCPD and CMXCLD.\n\n\n\nds.boxPlot(\"data$CSXRRA\")\n\nds.asFactor(input.var.name = \"data$CMXCLD\", newobj.name = \"CMXCLD_factor\")\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCLD_factor)\")\nds.asFactor(input.var.name = \"data$CMXCPD\", newobj.name = \"CMXCPD_factor\")\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCPD_factor)\")\n\nds.boxPlot(x = \"data\", variables = \"CSXRRA\", group = \"CMXCPD_factor\")\nds.boxPlot(x = \"data\", variables = \"CSXRRA\", group = \"CMXCPD_factor\", group2 = \"CMXCLD_factor\")\n\nds.table(\"data$CMXCPD\", \"data$CMXCLD\")"
  },
  {
    "objectID": "exercises.html#exercise-6-statistical-models",
    "href": "exercises.html#exercise-6-statistical-models",
    "title": "Exercises",
    "section": "Exercise 6: Statistical models",
    "text": "Exercise 6: Statistical models\nUsing the connection object from the Exercise 1 and 2:\n\nExerciseSolution\n\n\n\nFit a GLM (gaussian) with the model LBXSC3SIHn ~ TRXIS. (Model for illustrating purpose, not to answer any cientific question)\nFit a GLM (Poisson) with the model DATLGT ~ COUNTS (COUNTS is the variable created on the exercise 4)\n\n\n\nds.glm(formula = \"LBXSC3SIHn ~ TRXIS\", data = \"data\", family = \"gaussian\")\nds.glm(formula = \"DATLGT ~ COUNTS\", data = \"data\", family = \"poisson\")"
  },
  {
    "objectID": "exercises.html#exercise-7-extra",
    "href": "exercises.html#exercise-7-extra",
    "title": "Exercises",
    "section": "Exercise 7: Extra",
    "text": "Exercise 7: Extra\nUsing the connection object from the Exercise 1 and 2:\n\nExerciseSolution\n\n\n\nDo a variable selection (Lasso regression) without the date variables.\nDo a survival analysis with the variables of the Lasso regression.\n\n\n\nlibrary(dsSurvivalClient)\ntypes <- lapply(ds.colnames(\"data_complete\")[[1]], function(x){\n  ds.class(paste0(\"data_complete$\", x))[[1]][1]\n})\ntypes <- unlist(types)\n`%notin%` <- Negate(`%in%`)\nindexes_to_remove <- which(types %notin% \"numeric\")\ntimes <- ds.dim(\"data_complete\")[[1]][1]\nds.rep(x1 = 1,\n            times = times,\n            source.times = \"c\",\n            source.each = \"c\",\n            newobj = \"ONES\")\n\nds.dataFrameSubset(df.name = 'data_complete',  V1.name = \"ONES\",  V2.name = \"ONES\",  Boolean.operator = \"==\",keep.cols = NULL,  rm.cols = indexes_to_remove,  keep.NAs = NULL,  newobj = 'data_complete_numeric',  datasources = connections, notify.of.progress = FALSE)\nds.assign(toAssign='data_complete_numeric$DSXOS_recoded_num', newobj='Y', datasources = connections)\n\nds.dataFrameSubset(df.name = 'data_complete_numeric',  V1.name = \"ONES\",  V2.name = \"ONES\",  Boolean.operator = \"==\",keep.cols = NULL,  rm.cols = c(1, 2, 5, 6, 9, 110, 111, 112),  keep.NAs = NULL,  newobj = 'X',  datasources = connections, notify.of.progress = FALSE)\nds.asMatrix(x.name = 'Y', newobj = 'Y')\nds.asMatrix(x.name = 'X', newobj = 'X')\n\nopts=list();opts$init=0; opts$maxIter=10; opts$tol=0.01; opts$ter=2;\n\nset.seed(123)\nm1=dsMTLClient::ds.LS_Lasso(X='X', Y='Y', lam=0.5, C=0, opts, datasources=connections, nDigits=15)\n\nvariables_interest <- ds.colnames(\"X\")[[1]][which(m1$w!=0)]\n\nds.make(toAssign = \"data$DSXOS_recoded_num\", newobj = \"EVENT\")\nds.make(toAssign = \"data$DATLGT\", newobj = \"SURVTIME\")\n\nformula <- paste0(\"survival::Surv(time=SURVTIME,event=EVENT)~\",\n                  paste(\"data$\", variables_interest, collapse = \"+\", sep = \"\"))\n\ndsSurvivalClient::ds.coxph.SLMA(formula = formula,\n                                dataName = 'data',\n                                datasources = connections)\n\ndsSurvivalClient::ds.survfit(formula = formula, objectname = \"scurves\")\nlibrary(survival)\ndsSurvivalClient::ds.plotsurvfit(formula = \"scurves\")"
  }
]