[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataSHIELD Workshop",
    "section": "",
    "text": "This website has been created to host the materials and exercises for the ‘Utilization of the unCoVer toolbox for covidー19 data analysis’, hosted at the UPM-Montegancedo Campus the 6th of May, 2022.\nOn it you will find reading materials, setup tutorials, the workshop indications and practical exercises.\n\n\nBefore the workshop we suggest the atendants to take a look at the “Environment setup” and “Get up to speed” sections. This way they will have their computers with the right software installed to follow the workshop.\n\n\n\n\n\n\nTime\nTopic\n\n\n\n\n11:00\nWelcome\n\n\n11:05\nunCoVer toolbox: presentation of the infrastructure\n\n\n11:45\nQ&A\n\n\n12:00\nCase Study (I): Part 0, 1 and 2\n\n\n13:15\nLunch\n\n\n14:15\nCase Study (II): Part 3 and 4\n\n\n15:30\nCoffee break\n\n\n15:45\nCase Study (III): Part 5\n\n\n17:25\nClosing\n\n\n17:30\nEnd\n\n\n\n\n\n\nThis project is funded by the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreement No 101016216. Material developed at ISGlobal-Barcelona (BRGE) by Juan R. González and Xavier Escribà Montagut"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Environment setup",
    "section": "",
    "text": "Along this section, we will go over the R packages (and versions) required to navigate through the workshop. We recommend to use RStudio as the integrated development environment (IDE) to use R, although any other IDE can be used. We suggest to use R version > 4.0."
  },
  {
    "objectID": "setup.html#r-packages",
    "href": "setup.html#r-packages",
    "title": "Environment setup",
    "section": "R packages",
    "text": "R packages\nThe required R packages are the following:\n\nopalr 3.1.0: To login to an Opal server using R and retrieve information of the server (projects, resources, tables, etc).\nDSI 1.4.0: The DataSHIELD Interface (DSI) handles the connections to the databases.\nDSOpal 1.3.0: DSOpal is an extension of DSI to connect to to Opal servers.\ndsBaseClient 6.1.1: Implementation of the base R functions (Example: Base package function as.factor is implemented as ds.asFactor).\ndsHelper 0.4.12: dsHelper is a compedium of wrappers for dsBaseClient functions that simplify some common operations to maintain sanity."
  },
  {
    "objectID": "setup.html#install-guide",
    "href": "setup.html#install-guide",
    "title": "Environment setup",
    "section": "Install guide",
    "text": "Install guide\n\n\n\n\n\n\nThe proposed install guide has been tested on a clean installation with R 4.0.4 and RStudio 2022.02.2 Build 485; if any errors occur, please consider using a clean install or refer to the official R documentation regarding the errors you are getting. If you are facing permissions issues, contact your system administrator.\n\n\n\nTo install the required packages we will need the devtools package.\n\ninstall.packages(\"devtools\")\n\nAfterwards, we can install the packages:\n\ndevtools::install_version(\"opalr\", version = \"3.1.0\")\ndevtools::install_version(\"DSI\", version = \"1.4.0\")\ndevtools::install_version(\"DSOpal\", version = \"1.3.0\")\ndevtools::install_github(\"datashield/dsBaseClient\", \"6.1.1\")\ndevtools::install_github(\"lifecycle-project/ds-helper\", \"0.4.12\")\n\n\n\n\nInstall console"
  },
  {
    "objectID": "help.html",
    "href": "help.html",
    "title": "Get up to speed: Useful reads",
    "section": "",
    "text": "Along this workshop, there are some details regarding DataSHIELD and “resources” that are not explained in detail, it is expected that the reader is familiar with them. If that is not the case, there are other free online books/papers with that knowledge.\n\nDataSHIELD paper: Description of what is DataSHIELD.\nDataSHIELD wiki: Materials about DataSHIELD including:\n\nBeginner material\nRecorded DataSHIELD workshops\nInformation on current release of DataSHIELD\n\nresource book: In this book you will find information about:\n\nDataSHIELD (Section 5)\nWhat are resources (Section 6/7)"
  },
  {
    "objectID": "help.html#opal",
    "href": "help.html#opal",
    "title": "Get up to speed: Useful reads",
    "section": "Opal",
    "text": "Opal\nWe will be interacting with DataSHIELD through a data warehouse called Opal. This is the server that will handle the authentication of our credentials, storage of data and “resources” and will provide an R server where the non-disclosive analysis will be conducted. Information about it can also be foun online:\n\nOpal papers 1; 2\nOpal documentation"
  },
  {
    "objectID": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "href": "help.html#resources-a-very-simple-explanation-without-any-technicalities",
    "title": "Get up to speed: Useful reads",
    "section": "“resources”: A very simple explanation without any technicalities",
    "text": "“resources”: A very simple explanation without any technicalities\nIt is quite important to have a solid understanding of what are the “resources” and how we work with them, since we will be using them to load our data on the R sessions. For that reason we included a very brief description of them without using technicalities.\nThe “resources” can be imagined as a data structure that contains the information about where to find a data set and the access credentials to it; we as DataSHIELD users are not able to look at this information (it is privately stored on the Opal server), but we can load it into our remote R session to make use of it. Following that, the next step comes naturally.\nOnce we have in an R session the information to access a dataset (an table for example) we have to actually retrieve it on the remote R session to analyze it. This step is called resolving the resource.\nThose two steps can be identified on the code we provide as the following:\nLoading the information of a “resource”:\n\nDSI::datashield.assign.resource(conns, \"resource\", \"resource.path.in.opal.server\")\n\nResolving the “resource”:\n\nDSI::datashield.assign.expr(conns, \"resource.resolved\", expr = as.symbol(\"as.resource.data.frame(resource)\"))\n\nThis toy code would first load the “resource” on a variable called resource and it would retrieve the information it contains and assign it to a variable called resource.resolved."
  },
  {
    "objectID": "workshop_part0.html",
    "href": "workshop_part0.html",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "",
    "text": "To begin, we will load the libraries that will allow us to connect to the Opal analysis servers.\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)"
  },
  {
    "objectID": "workshop_part0.html#log-in-to-the-study-servers",
    "href": "workshop_part0.html#log-in-to-the-study-servers",
    "title": "Part 0: Connecting to the analysis servers",
    "section": "Log in to the study servers",
    "text": "Log in to the study servers\nWe begin by creating the connection object, to do that we will use the DSI package. It is at this stage that we can decide whether only connect to a single study server or to multiple. If we connect to multiple study servers we can make use of the pooled functionalities of some DataSHIELD functions.\n\nbuilder <- DSI::newDSLoginBuilder()\nbuilder$append(server = \"hm_hospitales\",\n               url = \"https://192.168.1.50:9002\",\n               user = \"user_analisis\", password = \"Ekfl07UUgz\")\nbuilder$append(server = \"sc_verona\",\n               url = \"https://192.168.1.50:8890\",\n               user = \"user_analisis\", password = \"Ekfl07UUgz\")\nbuilder$append(server = \"umf_cluj\",\n               url = \"https://192.168.1.200:8005\",\n               user = \"user_analisis\", password = \"Ekfl07UUgz\")\nbuilder$append(server = \"test_server\", # This server will fail!\n               url = \"https://192.168.1.1:8888\",\n               user = \"test\", password = \"pass\")\nlogindata <- builder$build()\n\nWe just created the logindata object, which contains all the login information, the next step is to use this information and actually connect to the servers.\n\nconnections <- DSI::datashield.login(logins = logindata)\n\n\nLogging into the collaborating servers\n\n\nError in curl::curl_fetch_memory(url, handle = handle): schannel: next InitializeSecurityContext failed: SEC_E_UNTRUSTED_ROOT (0x80090325) - La cadena de certificación fue emitida por una entidad en la que no se confía.\n\n\nThe error we are getting is due the study server having no SSL certificate, by default R refuses to connect to endpoints without SSL certificates. We do not need to know what an SSL certificate is or why is it important.\nWe can maually disable this limitation of R so that it accepts to connect.\n\nlibrary(httr);set_config(config(ssl_verifypeer = 0L))\n\nAnd now we successfully connect to the study servers.\n\nconnections <- DSI::datashield.login(logins = logindata, failSafe = TRUE)\n\n\nLogging into the collaborating servers\n\n\nWarning: test_server is excluded because login connection failed\n\n\nWe can see that if one of the servers fails to connect we get a warning message, but the successful connections get stored into the connections object."
  },
  {
    "objectID": "workshop_part1.html",
    "href": "workshop_part1.html",
    "title": "Part 1: Loading the data",
    "section": "",
    "text": "Now that we are connected to the study servers, we have to load the data on the remote R sessions."
  },
  {
    "objectID": "workshop_part1.html#loading-the-resources",
    "href": "workshop_part1.html#loading-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Loading the resources",
    "text": "Loading the resources\nWe will begin by loading the resources. To do so, we have to know the path to them in the Opal servers, if we do not know it, there are two different ways of finding out.\n\nOpal UI\nWe can use the web user interface of Opal. To do that, just go to your browser of choice and navigate to the server URL. Login with the same credentials you are using for DataSHIELD.\n\n\n\nOpal UI landing page\n\n\nOnce we login, we have to navigate to the Projects tab, there we will find the available projects on the Opal server, we have to write down the name of the project we are interested on using.\n\n\n\nOpal UI Projects page\n\n\nWe then click on the project of interest. On the resources tab we will see the available resources. We have to write down the resource of interest.\n\n\n\nOpal UI Available resources\n\n\n\n\nUsing R console\nWe can use the opalr package to interact with the Opal server and retrieve information about it. The information we are interested on is the available projects and the resources they contain.\nFirst we login to one Opal, if we are using multiple servers, we have to perform the same procedure for each one; we will illustrate the HM Hospitals server only.\n\no <- opalr::opal.login(username = \"user_analisis\", \n                  password = \"Ekfl07UUgz\", \n                  url = \"https://192.168.1.50:9002\")\n\nThen, we look for the available projects.\n\nopalr::opal.projects(o)\n\n  name title tags                  created               lastUpdate\n1 FiHM  FiHM   NA 2022-02-10T10:29:06.000Z 2022-02-10T10:29:06.000Z\n\n\nAnd finally, we look for the available resources inside this project.\n\nopalr::opal.resources(o, \"FiHM\")\n\n             name project\n1 harmonized_data    FiHM\n                                                                   url format\n1 opal+http://opal:8080/ws/files/projects/FiHM/HM_Harmonized_Dates.csv    csv\n               created              updated\n1 2022-04-25T09:51:27Z 2022-04-25T09:51:27Z\n\n\n\nopalr::opal.logout(o)\n\n\n\nProjects and resources names\nIf we perform the same operations for all three study centers we are using in this workshop, we can obtain the following information:\n\n\n\n\n\n\n\n\n\nStudy center\nURL\nProject Name\nResource Name\n\n\n\n\nHM Hospitales\nhttps://192.168.1.50:9002\nFiHM\nharmonized_data\n\n\nSacrocuore Verona\nhttps://192.168.1.50:8890\nS_uncover\nverona\n\n\nUMF Cluj\nhttps://192.168.1.200:8005\nvUMF_Cluj\nRomania\n\n\n\n\n\nLoading the resources in the remote R sessions\nNow we have to tell each study center to load the correspondent resource. What we have created on the previous part is a connection object that contains the connections to each study server. We can see how it looks.\n\nconnections\n\n$hm_hospitales\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"hm_hospitales\"\n\nSlot \"opal\":\nurl: https://192.168.1.50:9002 \nname: hm_hospitales \nversion: 4.0.3 \nusername: user_analisis \nprofile:  \n\n\n$sc_verona\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"sc_verona\"\n\nSlot \"opal\":\nurl: https://192.168.1.50:8890 \nname: sc_verona \nversion: 4.2.8 \nusername: user_analisis \nprofile:  \n\n\n$umf_cluj\nAn object of class \"OpalConnection\"\nSlot \"name\":\n[1] \"umf_cluj\"\n\nSlot \"opal\":\nurl: https://192.168.1.200:8005 \nname: umf_cluj \nversion: 4.0.3 \nusername: user_analisis \nprofile:  \n\n\nIt is basically a typical R list, therefore we can interact with a single server (for example the Verona one) by using connections[2]. We will do that to send the appropiate information to each study server. The syntaxis to specifiy the path of the resource follows the structure project.resource_name.\n\nDSI::datashield.assign.resource(connections$hm_hospitales, \"resource\", \"FiHM.harmonized_data\")\nDSI::datashield.assign.resource(connections$sc_verona, \"resource\", \"S_uncover.verona\")\nDSI::datashield.assign.resource(connections$umf_cluj, \"resource\", \"UMF_Cluj.Romania\")\n\nNow, each study server has an object called resource with the correspondent information. So in the following function calls we don’t have to send different information to each server."
  },
  {
    "objectID": "workshop_part1.html#resolving-the-resources",
    "href": "workshop_part1.html#resolving-the-resources",
    "title": "Part 1: Loading the data",
    "section": "Resolving the resources",
    "text": "Resolving the resources\nThe next step is to use the information of the resource object and load the dataset that it points to. In order do so, we just have to “resolve” this object. We do that with the following function.\n\nDSI::datashield.assign.expr(conns = connections, symbol = \"data\", \n                            expr = \"as.resource.data.frame(resource)\")\n\nWe have created an object called data. This object contains a dataframe with the data we will use to perform our analysis. We can check if that is true and the dimensions of this dataframe.\n\nds.class(\"data\")\n\n$hm_hospitales\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n$sc_verona\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n$umf_cluj\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\nds.dim(\"data\")\n\n$`dimensions of data in hm_hospitales`\n[1] 6864  219\n\n$`dimensions of data in sc_verona`\n[1] 1515   45\n\n$`dimensions of data in umf_cluj`\n[1] 999 132\n\n$`dimensions of data in combined studies`\n[1] 9378  219"
  },
  {
    "objectID": "workshop_part2.html",
    "href": "workshop_part2.html",
    "title": "Part 2: Data validation",
    "section": "",
    "text": "Now that we have the data loaded, we can check if the data is compliant with the ranges described on the codebook. (IS IT PUBLIC/CAN WE LINK IT HERE?). On this step we can check that the categorical variables are properly encoded and that the numerical variables have the appropriate range. Regarding the numerical variables, we will not be checking the range, we will be looking the 5%/95% interquartile range, as there is no DataSHIELD function to output maximums and minimums."
  },
  {
    "objectID": "workshop_part2.html#columns-available",
    "href": "workshop_part2.html#columns-available",
    "title": "Part 2: Data validation",
    "section": "Columns available",
    "text": "Columns available\nFirst, we check the columns available on the loaded data.\n\ncolnames_servers <- ds.colnames(\"data\")\n\nSince not all study centers have collected the same variables, we can use the following script to see the common variables.\n\nReduce(intersect, colnames_servers)\n\n [1] \"DMRAGEYR\"       \"DATAD\"          \"CMXCVD\"         \"CMXCVD_numeric\"\n [5] \"CMXHT\"          \"CMXHT_numeric\"  \"CMXCKD\"         \"CMXCKD_numeric\"\n [9] \"CMXCLD\"         \"CMXCLD_numeric\" \"CMXCPD\"         \"CMXCPD_numeric\"\n[13] \"CMXRHE\"         \"CMXRHE_numeric\"\n\n\nAnother scenario may be the one where we know our variables of interest and want to check whether they are or not on the study centers. To do that we can use a function from the dsHelper package.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndsHelper::dh.classDiscrepancy(df = \"data\", vars = c(\"DSXOS\", \"CMXCLD\", \"RFXONC\"), conns = connections)\n\n# A tibble: 3 x 5\n  variable discrepancy hm_hospitales sc_verona umf_cluj \n  <chr>    <chr>       <chr>         <chr>     <chr>    \n1 DSXOS    yes         character     NULL      character\n2 CMXCLD   no          character     character character\n3 RFXONC   yes         NULL          character character\n\n\nThis function gives us information about the class of the selected variables on each study server. Also, when a certain variable is not available, it shows as NULL class. So with this function we can get information about data availability and harmonization."
  },
  {
    "objectID": "workshop_part2.html#dropping-a-study-server",
    "href": "workshop_part2.html#dropping-a-study-server",
    "title": "Part 2: Data validation",
    "section": "Dropping a study server",
    "text": "Dropping a study server\nOnce we are aware of the available variables, we might be interested on dropping one of the connections (or multiple). To do so, we have to close the desired connection. We will illustrate that we are dropping the Madrid study center.\n\ndatashield.logout(connections$hm_hospitales)\n\nNow the connection is closed, so we can remove the HM Hospitales server from our connections list.\n\nconnections$hm_hospitales <- NULL\n\nFrom this point onwards, the connections object will only access the Verona and Cluj study servers."
  },
  {
    "objectID": "workshop_part2.html#categorical-variables",
    "href": "workshop_part2.html#categorical-variables",
    "title": "Part 2: Data validation",
    "section": "Categorical variables",
    "text": "Categorical variables\nWe compare against the codebook and see that we have the CMXCVD variable, which should have the categories: 0 and 1 (plus missing).\n \nFirst, we can check the class of this variable.\n\nds.class(\"data$CMXCVD\")\n\n$sc_verona\n[1] \"character\"\n\n$umf_cluj\n[1] \"character\"\n\n\nWe might have expected to have a variable of class factor, later on this workshop we will see how to transform a variable to a factor, for the moment we do not have to worry. Having a character variable is enough for the data validation.\nFollowing that, we can extract an uni-dimensional contingency table of the variable. This way we will obtain the different categories of the variable and their counts. Aside, we will obtain the count of missings (NA) for this variable.\n\nCMSCVD_table <- ds.table(\"data$CMXCVD\")\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\nCMSCVD_table$output.list$TABLES.COMBINED_all.sources_counts\n\ndata$CMXCVD\n Yes   No   NA \n 248  143 2123 \n\nCMSCVD_table$output.list$TABLE_rvar.by.study_counts\n\n           study\ndata$CMXCVD sc_verona umf_cluj\n        Yes       139      109\n        No          0      143\n        NA       1376      747\n\n\nThis variable complies with the codebook."
  },
  {
    "objectID": "workshop_part2.html#numerical-variables",
    "href": "workshop_part2.html#numerical-variables",
    "title": "Part 2: Data validation",
    "section": "Numerical variables",
    "text": "Numerical variables\nFrom previously shown codebook screenshot, we can see that we also have the DMRAGEYR variable, which should be numeric. We can confirm that.\n\nds.class(\"data$DMRAGEYR\")\n\n$sc_verona\n[1] \"numeric\"\n\n$umf_cluj\n[1] \"numeric\"\n\n\nAs previously mentioned, we will check the 5%/95% interquartile range to see the range of the variable.\n\nds.quantileMean(\"data$DMRAGEYR\", type = \"split\")\n\n$sc_verona\n      5%      10%      25%      50%      75%      90%      95%     Mean \n16.70000 22.40000 38.00000 53.00000 68.00000 78.00000 82.30000 52.06601 \n\n$umf_cluj\n    5%    10%    25%    50%    75%    90%    95%   Mean \n40.450 46.900 54.250 65.000 72.000 80.100 84.000 63.832 \n\n\nThis is not the most useful way to check the range of a variable. There is an alternative provided by the dsHelper package, which is to exatract the (noise-perturbed) variable which would be used for the scatter plot. Depending on the security parameters of the Opal server, the noise added could be too severe to be useful, but with the default settings it serves as a great alternative.\n\nvariable <- dh.getAnonPlotData(df = \"data\", var_1 = \"DMRAGEYR\")\nhead(variable)\n\n# A tibble: 6 x 2\n  cohort    DMRAGEYR\n  <chr>        <dbl>\n1 sc_verona     70.0\n2 sc_verona     43.0\n3 sc_verona     54.0\n4 sc_verona     45.0\n5 sc_verona     67.0\n6 sc_verona     59.0\n\nvariable %>% \n  dplyr::group_by(cohort) %>% \n  dplyr::summarise(range = paste(round(range(DMRAGEYR)), collapse = \", \"))\n\n# A tibble: 3 x 2\n  cohort    range \n  <chr>     <chr> \n1 combined  10, 94\n2 sc_verona 10, 94\n3 umf_cluj  31, 92\n\n\nFinally, we can also check the amount of missings of the numerical variable.\n\nds.numNA(\"data$DMRAGEYR\")\n\n$sc_verona\n[1] 0\n\n$umf_cluj\n[1] 749"
  },
  {
    "objectID": "workshop_part4.html",
    "href": "workshop_part4.html",
    "title": "Part 3: Data wranggling",
    "section": "",
    "text": "We are now in the position to start working with the data. The first step is to learn how to manipulate it."
  },
  {
    "objectID": "workshop_part4.html#data-classes",
    "href": "workshop_part4.html#data-classes",
    "title": "Part 3: Data wranggling",
    "section": "Data classes",
    "text": "Data classes\nAll the tools to perform data transformations are the following:\n\nds.asCharacter: Useful to convert numerical values that we want as characters.\nds.asDataMatrixr: Useful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Maintains the original class for all columns.\nds.asMatrixUseful to convert data.frames into matrices, as some functions do not accept data frames as inputs. Converts all columns into character class.\nds.asFactor: May cause disclosure issues if we try to convert a continuous variable. Very useful when the loaded categorical variables are represented as character instead of factor.\nds.asInteger: Useful when we require integer values, as the class numeric can’t guarantee it.\nds.asList: Rarely used.\nds.asLogical: To have bool variables. Similar to having factor variables.\nds.asNumeric: Useful to convert columns that have been interpreted as character class but we want them as numbers.\n\nWhen we use all this functions, we will create a new object on the study servers. If we are dealing with a data.frame and want to include this new column we created into it, we can use the following code.\n\nds.asFactor(input.var.name = \"data$CMXCVD\", newobj.name = \"CMXCVD_factor\")\n\n$all.unique.levels\n[1] \"Yes\" \"No\" \n\n$return.message\n[1] \"Data object <CMXCVD_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCVD_factor)\")\n\nWe will overwrite the input data object with an added column.\n\ntail(ds.colnames(\"data\")[[1]])\n\n[1] \"SMXSBA_numeric\"   \"SMXNAA_numeric\"   \"DMRGENDR_numeric\" \"DATAD_year\"      \n[5] \"DATAD_year_day\"   \"CMXCVD_factor\""
  },
  {
    "objectID": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "href": "workshop_part4.html#complex-recoding.-number-of-comorbidities",
    "title": "Part 3: Data wranggling",
    "section": "Complex recoding. Number of comorbidities",
    "text": "Complex recoding. Number of comorbidities\nGiven the available variables on our dataset, we might be interested on recoding variables or creating new variables as combinations of the existing ones. This is exactly what we will do here. We will take the comorbities: CMXHT, CMXCVD, CMXCPD, CMXCKD, CMXCLD and RFXONC.\nWe will create a new variable called CMXCOM that quantifies how many comorbidities an individual has. We will have four categories:\n\n0 comorbidities\n1 comorbidities\n2 comorbidities\n3+ comorbidities\n\nThe process to achieve this is not as easy as straightforward as it would be using base R functions on our computer.\nTo begin, we will take a look at how the variables are encoded. Looking at the codebook they should all be dicotomous variables encoded as \"Yes\" / \"No\". We can verify that with the functions we’ve covered at the Part 2 of this workshop.\n\nds.table(\"data$CMXHT\")$output.list$TABLES.COMBINED_all.sources_counts\n\n\n Data in all studies were valid \n\nStudy 1 :  No errors reported from this study\nStudy 2 :  No errors reported from this study\n\n\ndata$CMXHT\n Yes   No   NA \n 436  101 1977 \n\n\nNow, we will recode all the variabled so they are coded as 0 / 1.\n\nvariables <- c(\"CMXHT\", \"CMXCVD\", \n               \"CMXCPD\", \"CMXCKD\", \"CMXCLD\", \"RFXONC\")\n\nfor (x in variables){\n  ds.recodeValues(var.name = paste0(\"data$\", x), \n                  values2replace.vector = c(\"Yes\", \"No\"), \n                  new.values.vector = c(1, 0),\n                  newobj = paste0(x, \"_recoded\"))\n}\n\nWe have created a new object for each comorbiditie. The problem we now have is that the objects are of class character, because the ds.recodeValues function does not change the class.\n\nds.class(\"CMXHT_recoded\")\n\n$sc_verona\n[1] \"character\"\n\n$umf_cluj\n[1] \"character\"\n\n\nSince we want to count the numbers of comorbidities for each individual, it is important that we have a numeric variable. As we have prevously seen we can easily obtain this.\n\nfor (x in variables){\n  ds.asNumeric(x.name = paste0(x, \"_recoded\"), \n               newobj = paste0(x, \"_recoded_num\"))\n}\n\n\nds.class(\"CMXHT_recoded_num\")\n\n$sc_verona\n[1] \"numeric\"\n\n$umf_cluj\n[1] \"numeric\"\n\n\nNow we have a collection of variables encoded as we want and ready to be combined. First, we will join them on a data.frame.\n\nds.dataFrame(x = paste0(variables, \"_recoded_num\"), \n             newobj = \"joint_comorbidities\")\n\n$is.object.created\n[1] \"A data object <joint_comorbidities> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<joint_comorbidities> appears valid in all sources\"\n\n\nOn the server we have created a table that looks something like this:\n\n\n\n\n\n\n\n\n\n\nid\nRFXOB_recoded_num\nCMXDI_recoded_num\nCMXHT_recoded_num\n…\n\n\n\n\nIndividual 1\n0\n0\n0\n…\n\n\nIndividual 2\n1\n1\n1\n…\n\n\n…\n…\n…\n…\n…\n\n\n\nThe variable we want to create is the amount of comorbidities by individual, therefore we have to perform rowSums.\n\nds.rowColCalc(x = \"joint_comorbidities\", \n              operation = \"rowSums\", \n              newobj = \"new_variable\")\n\nWe are almost done, we will convert our new variable to be a factor.\n\nds.asFactor(input.var.name = \"new_variable\", \n            newobj.name = \"new_variable_factor\")\n\n$all.unique.levels\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\"\n\n$return.message\n[1] \"Data object <new_variable_factor> correctly created in all specified data sources\"\n\n\nWe can see that when we call the function ds.asFactor we receive the actual levels of the variable. We have said that the levels we actually want are 0, 1, 2, 3+.\nTo achieve this we just have to re-code the levels.\n\nds.recodeValues(var.name = \"new_variable_factor\", \n                values2replace.vector = c(\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"),\n                new.values.vector = c(\"0\", \"1\", \"2\",\"3+\", \"3+\", \"3+\",\"3+\"), \n                newobj = \"CMXCOM\")\n\n$is.object.created\n[1] \"A data object <CMXCOM> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<CMXCOM> appears valid in all sources\"\n\n\nFinally, we merge the new variable to our original data, and we are finished.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXCOM)\")"
  },
  {
    "objectID": "workshop_part4.html#complete-cases",
    "href": "workshop_part4.html#complete-cases",
    "title": "Part 3: Data wranggling",
    "section": "Complete cases",
    "text": "Complete cases\nReal data tends to have missing values, however, some specific tools or functions expect complete data as input. For that reason there is a function specifically for that. Let’s suppose we want to have the complete cases for the variables DATAD and CMXHT. First we join them on a new data.frame.\n\nds.dataFrame(x = c(\"data$DATAD\", \"data$CMXHT\"), newobj = \"data_subset\")\n\n$is.object.created\n[1] \"A data object <data_subset> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset> appears valid in all sources\"\n\n\nNow we can get the complete cases.\n\nds.completeCases(x1 = \"data_subset\", newobj = \"data_subset_ccases\")\n\n$is.object.created\n[1] \"A data object <data_subset_ccases> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<data_subset_ccases> appears valid in all sources\"\n\n\nWe can check the dimensions before and after.\n\nds.dim(\"data_subset\")\n\n$`dimensions of data_subset in sc_verona`\n[1] 1515    2\n\n$`dimensions of data_subset in umf_cluj`\n[1] 999   2\n\n$`dimensions of data_subset in combined studies`\n[1] 2514    2\n\nds.dim(\"data_subset_ccases\")\n\n$`dimensions of data_subset_ccases in sc_verona`\n[1] 269   2\n\n$`dimensions of data_subset_ccases in umf_cluj`\n[1] 252   2\n\n$`dimensions of data_subset_ccases in combined studies`\n[1] 521   2"
  },
  {
    "objectID": "workshop_part4.html#dates-what-to-do",
    "href": "workshop_part4.html#dates-what-to-do",
    "title": "Part 3: Data wranggling",
    "section": "Dates: What to do…",
    "text": "Dates: What to do…\nThe unCoVer consortia have many variables that correspond to dates. At the moment DataSHIELD has no functions to work with dates. We could develop a package to work with them if researchers can benefit from that."
  },
  {
    "objectID": "workshop_part3.html",
    "href": "workshop_part3.html",
    "title": "Part 4: Descriptive analysis",
    "section": "",
    "text": "We have already checked that our data is compliant with the codebook. Now we can proceed with our analysis using the selected variables. To begin, we will perform a brief descriptive analysis, calculating some statistics, tables of contingence and doing some graphical visualizations."
  },
  {
    "objectID": "workshop_part3.html#descriptive-statistics",
    "href": "workshop_part3.html#descriptive-statistics",
    "title": "Part 4: Descriptive analysis",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\nThere is a collection of DataSHIELD functions to get the descriptive statistics of a variable. Those are the functions ds.var, ds.mean, ds.table among others. However, there is a function from the dsHelper package that automatically performs all the function calls on the background, being a much more user-friendly alternative. The only downside is that it requires the categorical variables to be factors, we can easily create a new factor variable and add it as a new column to our data.\n\nds.asFactor(input.var.name = \"data$CMXHT\", newobj.name = \"CMXHT_factor\")\n\n$all.unique.levels\n[1] \"Yes\" \"No\" \n\n$return.message\n[1] \"Data object <CMXHT_factor> correctly created in all specified data sources\"\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, CMXHT_factor)\")\n\ndh.getStats(df = \"data\", vars = c(\"DMRAGEYR\", \"CMXHT_factor\"))\n\n$categorical\n# A tibble: 6 x 10\n  variable     cohort    category value cohort_n valid_n missing_n perc_valid\n  <chr>        <chr>     <fct>    <int>    <int>   <int>     <int>      <dbl>\n1 CMXHT_factor combined  Yes        436     2514     537      1977       81.2\n2 CMXHT_factor combined  No         101     2514     537      1977       18.8\n3 CMXHT_factor sc_verona Yes        285     1515     285      1230      100  \n4 CMXHT_factor sc_verona No           0     1515     285      1230        0  \n5 CMXHT_factor umf_cluj  Yes        151      999     252       747       59.9\n6 CMXHT_factor umf_cluj  No         101      999     252       747       40.1\n# ... with 2 more variables: perc_missing <dbl>, perc_total <dbl>\n\n$continuous\n# A tibble: 3 x 15\n  variable cohort    mean std.dev perc_5 perc_10 perc_25 perc_50 perc_75 perc_90\n  <chr>    <chr>    <dbl>   <dbl>  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 DMRAGEYR sc_vero~  52.1    20.0   16.7    22.4    38      53      68      78  \n2 DMRAGEYR umf_cluj  63.8    13.0   40.4    46.9    54.2    65      72      80.1\n3 DMRAGEYR combined  53.7    19.6   20.1    25.9    40.3    54.7    68.6    78.3\n# ... with 5 more variables: perc_95 <dbl>, valid_n <dbl>, cohort_n <dbl>,\n#   missing_n <dbl>, missing_perc <dbl>\n\ndh.getStats(df = \"data\", vars = c(\"SMXHEA\"))\n\n$categorical\nlist()\n\n$continuous\nlist()"
  },
  {
    "objectID": "workshop_part3.html#graphical-visualizations",
    "href": "workshop_part3.html#graphical-visualizations",
    "title": "Part 4: Descriptive analysis",
    "section": "Graphical visualizations",
    "text": "Graphical visualizations\nThere is a rich collection of functions to perform graphical visualization of the variables. All the visualizations can be of the pooled data or separated by study.\n\nBoxplot\nThe boxplot is one of the newest plots available on DataSHIELD. It uses the ggplot2 library, providing lots of customization options.\n\nCombined plot\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\")\n\n\n\n\n\n\nStudy separated plot\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\", type = \"split\")\n\n\n\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\nGroupings\nThere are grouping options on the boxplot function which are very useful to have greater insights of the data. We can group using factor variables, so in this example we will use the previously created SMXAPA_factor variable.\n\nds.boxPlot(x = \"data\", variables = \"DMRAGEYR\", group = \"CMXHT_factor\")\n\n\n\n\nWe could even perform a second grouping using the argument group2 and stating another factor variable.\n\n\n\nHistogram\nTo have an idea of the distribution of a variable we can use histograms.\n\nhistogram <- ds.histogram(\"data$DMRAGEYR\")\n\n\n\n\n\n\nScatter plot\nDataSHIELD has a scatter plot functionality that outputs noise-affected data points, depending on the security configuration of the Opal, the noise levels can make this plot to be hugely distorted.\n\nds.scatterPlot(x = \"data$DMRAGEYR\", y = \"data$CSXCHRA\", datasources = connections)\n\n\n\n\n[1] \"Split plot created\"\n\n\n\n\nHeatmap plot\nIn a similar fashion than the scatter plot, we can visualize a two dimensional distribution of the points but in this case by density.\n\nds.heatmapPlot(x = \"data$DMRAGEYR\", y = \"data$CSXCHRA\")"
  },
  {
    "objectID": "workshop_part3.html#dimensional-contingency-table",
    "href": "workshop_part3.html#dimensional-contingency-table",
    "title": "Part 4: Descriptive analysis",
    "section": "2-Dimensional contingency table",
    "text": "2-Dimensional contingency table\nOn the previous part, we have already seen that DataSHIELD has a function to calculate uni-dimensional contingency tables. This same function, can also be used for bi-dimensional contingency tables. It is important to note that this function is a little bit tricky sometimes, as it is quite common that the 2D contingency table has disclosive outputs, therefore we just get an error message.\n\nds.table(\"data$DMRAGEYR\", \"data$SMXDIA_numeric\")\n\n\n All studies failed for reasons identified below \n\n\nStudy 1 :  Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3 \n\nStudy 2 :  Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3 \n\n\n$validity.message\n[1] \"All studies failed for reasons identified below\"\n\n$error.messages\n$error.messages$sc_verona\n[1] \"Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3\"\n\n$error.messages$umf_cluj\n[1] \"Failed: at least one cell has a non-zero count less than nfilter.tab i.e. 3\"\n\n\nThere are other variables that do produce valid non-disclosive results.\n\nctable <- ds.table(\"data$CMXCVD\", \"data$CMXCKD\")$output.list$TABLES.COMBINED_all.sources_counts\nctable\n\n           data$CMXCKD\ndata$CMXCVD Yes  No  NA\n        Yes  20  89   0\n        No   11 132   0\n        NA    0   0 747\n\n\n\nFisher test\nGiven the calculated contingency table, we are now in the position to perform a Fisher’s exact test.\n\nstats::fisher.test(ctable)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  ctable\np-value < 2.2e-16\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "href": "workshop_part3.html#statistics-of-a-continuous-variable-grouped-by-a-categorical-variable",
    "title": "Part 4: Descriptive analysis",
    "section": "Statistics of a continuous variable grouped by a categorical variable",
    "text": "Statistics of a continuous variable grouped by a categorical variable\nWe may have interest on knowing the value of certain statistics of a variable when grouping with a category. As an example we will calculate the mean of the variable DMRAGEYR (Age) grouped by the variable CMXCOM (Number of comorbidities). Please note this is just an example, those two variables are not expected to be related.\nTo compute this statistic it is important that we have the categorical variable as a factor, otherwise the grouped statistic calculation will fail. Also, we have to create separate objects for the two variables, we can’t merge them into a data.frame and call them using the typical formulation data.frame$variable, for some reason it fails (to be reported as a bug to the core DataSHIELD team).\n\nds.assign(toAssign = \"data$DMRAGEYR\",\n            newobj =  \"DMRAGEYR\")\nds.assign(toAssign = \"data$CMXCOM\",\n            newobj =  \"CMXCOM\")\n\nds.tapply(X.name = \"DMRAGEYR\", INDEX.names = \"CMXCOM\", FUN.name = \"mean\")\n\n$sc_verona\n$sc_verona$Mean\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n 46.11960  65.47463  72.60241  78.80000 \n\n$sc_verona$N\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n     1087       335        83        10 \n\n\n$umf_cluj\n$umf_cluj$Mean\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n 53.53125  63.50704  68.98529  70.89362 \n\n$umf_cluj$N\n CMXCOM.0  CMXCOM.1  CMXCOM.2 CMXCOM.3+ \n       64        71        68        47 \n\n\nThis function can take other expressions on the argument FUN.name, more precisely:\n\n\"mean\"\n\"sd\"\n\"sum\"\n\"quantile\""
  },
  {
    "objectID": "workshop_part3.html#student-t-test",
    "href": "workshop_part3.html#student-t-test",
    "title": "Part 4: Descriptive analysis",
    "section": "Student t-test",
    "text": "Student t-test\nWe have two options to perform a t-test, and our choice will depend on whether we want to use pooled methods or we want to calculate the t-test on a single study server.\n\nPooled t-test\nTo perform a pooled t-test, we will use a collection of DataSHIELD functions to calculate the statistic.\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWhere:\n\n\\(\\bar{x}\\) is the observed mean of the sample.\n\\(s\\) is the standard devuation of the sample.\n\\(n\\) is the sample size.\n\nWe will use the following code to perform this operations:\n\nmean1 <- ds.mean(\"data$DMRAGEYR\", type = \"c\")$Global.Mean[1]\nmean2 <- ds.mean(\"data$CSXOSTA\", type = \"c\")$Global.Mean[1]\nsd1 <- sqrt(ds.var(\"data$DMRAGEYR\", type = \"c\")$Global.Variance[1])\nsd2 <- sqrt(ds.var(\"data$CSXOSTA\", type = \"c\")$Global.Variance[1])\nn1 <- ds.length(\"data$DMRAGEYR\")[[3]] - sum(unlist(ds.numNA(\"data$DMRAGEYR\")))\nn2 <- ds.length(\"data$CSXOSTA\")[[3]] - sum(unlist(ds.numNA(\"data$CSXOSTA\")))\n\n(mean1 - mean2) / sqrt(sd1^2/n1 + sd2^2/n2)\n\n[1] -91.58199\n\n\nYou can expect to see a wrapper for this code on a future dsHelper release (dsHelper::dh.ttest('sample1', 'sample2').\n\n\nSingle study t-test\nIf we are not interested on using pooled functionalities, it is much simpler to perform a t-test.\n\nDSI::datashield.aggregate(connections, \"t.test(data$DMRAGEYR, data$CSXOSTA)\")\n\n$sc_verona\n\n    Welch Two Sample t-test\n\ndata:  data$DMRAGEYR and data$CSXOSTA\nt = -88.928, df = 1557.2, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -47.07162 -45.03991\nsample estimates:\nmean of x mean of y \n 52.06601  98.12177 \n\n\n$umf_cluj\n\n    Welch Two Sample t-test\n\ndata:  data$DMRAGEYR and data$CSXOSTA\nt = -31.201, df = 377.97, p-value < 2.2e-16\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -30.74986 -27.10393\nsample estimates:\nmean of x mean of y \n 63.83200  92.75889"
  },
  {
    "objectID": "workshop_part3.html#analysis-of-variance-anova",
    "href": "workshop_part3.html#analysis-of-variance-anova",
    "title": "Part 4: Descriptive analysis",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)\nA student is developing a DataSHIELD function to perform ANOVAs. You can expect it to be available in the following months, probably as part of the dsML package."
  },
  {
    "objectID": "workshop_part5.html",
    "href": "workshop_part5.html",
    "title": "Part 5: Statistical models",
    "section": "",
    "text": "Now that we have covered the basics, we can begin to analyze the data in more depth. In this section we will go over some of the statistical models we can fit by using DataSHIELD. On this section we will only be using the Cluj study center."
  },
  {
    "objectID": "workshop_part5.html#logistical-regression",
    "href": "workshop_part5.html#logistical-regression",
    "title": "Part 5: Statistical models",
    "section": "Logistical regression",
    "text": "Logistical regression\nWe will perform some logistic regressions to see the association of different variables to the survival condition (DSXOS variable). To achieve this, we will have to apply some of the concepts we have already seen in order to prepare our data.\n\nPreparing the data\nFor the logistic regression, the variables we will use are the following:\n\nDSXOS: Character variable. Outcome status encoded as Deceased, Recovered and Transferred. To perform a logistic regression this variable will have to be re-encoded as 1/0 (case/control); the case being Deceased and the controls being Recovered and Transferred.\nCMXCOM: Factor variable. Number of comorbidities, encoded as 0, 1, 2 and 3+.\nCMXCLD: Character variable. Chronic liver disease, encoded as Yes and No.\nRFXONC: Character variable. Oncology, encoded as Yes and No.\n\nWe only have to do some data wrangling for the DSXOS variable, first we do the re-coding.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = c(\"Deceased\", \"Recovered\", \"Transferred\"), \n                new.values.vector = c(1, 0, 0),\n                newobj = \"DSXOS_recoded\")\n\nError: There are some DataSHIELD errors, list them with datashield.errors()\n\ndatashield.errors()\n\n$umf_cluj\n[1] \"Command 'recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\n    \\\"1,0,0\\\", NULL)' failed on 'umf_cluj': Error while evaluating 'is.null(base::assign('DSXOS_recoded', value={dsBase::recodeValuesDS(\\\"data$DSXOS\\\", \\\"Deceased,Recovered,Transferred\\\", \\\"1,0,0\\\", NULL)}))' -> Error : Error: values2replace.text argument too long (see nfilter.stringShort)\\n\"\n\n\nWe can see that the DataSHIELD filter is complaining that the length of the values2replace.vector is too large. To overcome this issue, we can perform separate function calls for each of the levels.\n\nds.recodeValues(var.name = \"data$DSXOS\", \n                values2replace.vector = \"Deceased\", \n                new.values.vector = 1,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Recovered\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\nds.recodeValues(var.name = \"DSXOS_recoded\", \n                values2replace.vector = \"Transferred\", \n                new.values.vector = 0,\n                newobj = \"DSXOS_recoded\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded> appears valid in all sources\"\n\n\nBy doing that we successfully created the object DSXOS_recoded, now we just have to convert it to numerical.\n\n# ds.table does not work, I am not sure why\nds.table1D(\"DSXOS_recoded\")\n\nWarning: 'ds.table1D' is deprecated.\nUse 'ds.table' instead.\nSee help(\"Deprecated\")\n\n\n$counts\n      DSXOS_recoded\n''              746\n0               220\n1                33\nTotal           999\n\n$percentages\n      DSXOS_recoded\n''            74.67\n0             22.02\n1              3.30\nTotal        100.00\n\n$validity\n[1] \"All tables are valid!\"\n\n\n\nds.asNumeric(x.name = \"DSXOS_recoded\", newobj = \"DSXOS_recoded_num\")\n\n$is.object.created\n[1] \"A data object <DSXOS_recoded_num> has been created in all specified data sources\"\n\n$validity.check\n[1] \"<DSXOS_recoded_num> appears valid in all sources\"\n\n\nFinally, we add it to our data.frame with the other covariates, and we are ready to perform the logistic regression.\n\nDSI::datashield.assign.expr(connections, \"data\", \"cbind(data, DSXOS_recoded_num)\")\n\n\n\nFitting the models\n\nPooled analysis\nNow we can fit the models. First, we will calculate the association of the outcome status to the covariables chronic liver disease and oncology.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCLD + RFXONC\", \n       data = \"data\", \n       family = \"binomial\")$coefficients\n\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -2.0743671  0.2180219 -9.5144888 1.826085e-21   -2.5016822\nCMXCLDYes    0.4811639  0.5395664  0.8917604 3.725214e-01   -0.5763668\nRFXONCYes    1.3471273  0.5861413  2.2982979 2.154484e-02    0.1983115\n            high0.95CI.LP      P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)     -1.647052 0.1116133     0.07574034       0.1615078\nCMXCLDYes        1.538695 1.6179565     0.56193629       4.6585055\nRFXONCYes        2.495943 3.8463603     1.21934216      12.1331717\n\n\nAnd we also calculate the association of the outcome status to the number of comorbidities.\n\nds.glm(formula = \"DSXOS_recoded_num ~ CMXCOM\", \n       data = \"data\", \n       family = \"binomial\")$coefficients\n\n              Estimate Std. Error    z-value      p-value low0.95CI.LP\n(Intercept) -2.4680995  0.4657780 -5.2988752 1.165182e-07   -3.3810077\nCMXCOM1      0.3731538  0.5977740  0.6242389 5.324707e-01   -0.7984617\nCMXCOM2      0.5709795  0.5871345  0.9724851 3.308093e-01   -0.5797829\nCMXCOM3+     1.2824759  0.5793410  2.2136804 2.685077e-02    0.1469883\n            high0.95CI.LP     P_OR low0.95CI.P_OR high0.95CI.P_OR\n(Intercept)     -1.555191 0.078125     0.03289432       0.1743377\nCMXCOM1          1.544769 1.452308     0.45002069       4.6868904\nCMXCOM2          1.721742 1.770000     0.56001991       5.5942654\nCMXCOM3+         2.417963 3.605556     1.15834044      11.2229794\n\n\n\n\nMeta-analysis\nTo fit the same models to be meta-analyzed, we just have to use a different function with the same structure.\n\nds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCLD + RFXONC\", \n           dataName = \"data\", \n           family = \"binomial\")$output.summary$study1$coefficients\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -2.0743671  0.2180173 -9.5146899 1.822557e-21\nCMXCLDYes    0.4811639  0.5395643  0.8917638 3.725195e-01\nRFXONCYes    1.3471273  0.5861394  2.2983052 2.154443e-02\n\n\n\nds.glmSLMA(formula = \"DSXOS_recoded_num ~ CMXCOM\", \n           dataName = \"data\", \n           family = \"binomial\")$output.summary$study1$coefficients\n\n\n\nSAVING SERVERSIDE glm OBJECT AS: < new.glm.obj >\n\n\n              Estimate Std. Error    z value     Pr(>|z|)\n(Intercept) -2.4680995  0.4657778 -5.2988776 1.165167e-07\nCMXCOM1      0.3731538  0.5977738  0.6242391 5.324706e-01\nCMXCOM2      0.5709795  0.5871343  0.9724853 3.308092e-01\nCMXCOM3+     1.2824759  0.5793409  2.2136810 2.685073e-02\n\n\nAt this moment, the consortia is using dsBaseClient 6.1.1. In the new version 6.2, there is a function to visualize the meta-analyzed coefficients using forestplots."
  },
  {
    "objectID": "workshop_part5.html#piecewise-exponential-regression",
    "href": "workshop_part5.html#piecewise-exponential-regression",
    "title": "Part 5: Statistical models",
    "section": "Piecewise Exponential Regression",
    "text": "Piecewise Exponential Regression\nThe same functions we just used to fit logistical regressions, can also fit piecewise exponential regressions. This is achieved by selecting the output family to be of type poisson. For the variable DATLGT (Length of stay in hospital), we can check that it does follow a poisson distribution.\n\nhistogram <- ds.histogram(\"data$DATLGT\")\n\nWarning: umf_cluj: 2 invalid cells\n\n\n\n\n\nAnd we can fit the model.\n\nds.glm(formula = \"DATLGT ~ CMXCOM\", \n       data = \"data\", \n       family = \"poisson\")$coefficients\n\n             Estimate Std. Error   z-value      p-value low0.95CI.LP\n(Intercept) 2.2363785 0.04085889 54.734193 0.000000e+00   2.15629656\nCMXCOM1     0.1885454 0.05368026  3.512379 4.441138e-04   0.08333405\nCMXCOM2     0.2703472 0.05355828  5.047719 4.471156e-07   0.16537487\nCMXCOM3+    0.1459219 0.06028338  2.420599 1.549495e-02   0.02776865\n            high0.95CI.LP EXPONENTIATED RR low0.95CI.EXP high0.95CI.EXP\n(Intercept)     2.3164605         9.359375      8.639084      10.139721\nCMXCOM1         0.2937568         1.207492      1.086905       1.341458\nCMXCOM2         0.3753195         1.310419      1.179835       1.455456\nCMXCOM3+        0.2640751         1.157106      1.028158       1.302226"
  }
]